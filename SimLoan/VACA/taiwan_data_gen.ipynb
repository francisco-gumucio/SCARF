{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from sklearn import metrics\n",
    "from functools import wraps\n",
    "from time import time\n",
    "import ot\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "n = 1000\n",
    "x_dim = 3\n",
    "hiddens = [x_dim + 1, 32, 64, 1]\n",
    "test_size = 0.2\n",
    "valid_size = 0.125\n",
    "batch_size = 100\n",
    "seq_len = 10\n",
    "epsilon = 0.5\n",
    "eps = str(epsilon).replace(\".\", \"_\")\n",
    "b = 0.2\n",
    "bet = str(0.2).replace(\".\", \"_\")\n",
    "l = 0.1\n",
    "c_hiddens = [x_dim + 1, 32, 64, 1]\n",
    "g_hidden_size = 64\n",
    "g_num_layers = 2\n",
    "d_hidden_size = 64\n",
    "d_num_layers = 2\n",
    "gan_epochs = 500\n",
    "train_vaca = 0\n",
    "dataset = 'taiwan'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "res_path = Path('../res')\n",
    "log_dir = res_path / f'taiwan'\n",
    "clf_path = log_dir / (f\"c_model_\" + \".pth\")\n",
    "efforts_path = log_dir / (f\"effort_model\" + \".pth\")\n",
    "gan_path = log_dir / (f\"gan_model_taiwan_\"  + str(gan_epochs) + \"_\" + str(g_hidden_size) + \"_eps_\" +eps+\"_b_\" + bet + \".pth\")\n",
    "re_clf_path = log_dir / (f\"dp_model-\" + str(6)+ \".pth\")\n",
    "tsne_path = log_dir / (f\"syn-tsne.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor(x):\n",
    "    return torch.FloatTensor(x)\n",
    "\n",
    "def to_tensor(z, x, y=None):\n",
    "    if torch.is_tensor(x):\n",
    "        zx = torch.cat([z, x], dim=1)\n",
    "    else:\n",
    "        zx = np.concatenate([z, x], axis=1)\n",
    "        zx = torch.FloatTensor(zx)\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = torch.FloatTensor(y)\n",
    "        return zx, y\n",
    "    return zx\n",
    "\n",
    "class TrueModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, seed=0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(hiddens[:-1], hiddens[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.optim = Adam(self.parameters())\n",
    "\n",
    "    def forward(self, zx):\n",
    "        return self.model(zx)\n",
    "\n",
    "    def predict(self, z, x):\n",
    "        zx = to_tensor(z, x)\n",
    "        pred = self(zx)\n",
    "        pred_y = pred.detach().round().cpu().numpy()\n",
    "        return pred_y\n",
    "\n",
    "    def fit(self, z, x, y, patience=10):\n",
    "        zx = to_tensor(z, x)\n",
    "        y = tensor(y)\n",
    "\n",
    "        epoch, counter = 0, 0\n",
    "        best_loss = float('inf')\n",
    "        while True:\n",
    "            pred = self(zx)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            \n",
    "            epoch += 1\n",
    "            if loss.item() <= best_loss:\n",
    "                best_loss = loss.item()\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == patience:\n",
    "                    break\n",
    "        #print(f\"TrueModel Fit Done in {epoch} epochs!\")\n",
    "\n",
    "    def sample(self, s, x, scale=0.8):\n",
    "        sx = to_tensor(s, x)\n",
    "        prob = self(sx)\n",
    "        y = torch.bernoulli(prob * scale)\n",
    "        return y.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demographic_parity(sensi, pred_y):\n",
    "\n",
    "    s0 = sum(sensi.squeeze() == 0)\n",
    "    s1 = sum(sensi.squeeze() == 1)\n",
    "    y0 = sum(pred_y.squeeze() == 0)\n",
    "    y1 = sum(pred_y.squeeze() == 1)\n",
    "    y1_s0 = sum(pred_y[sensi.squeeze() == 0].squeeze() == 1) / s0\n",
    "    y1_s1 = sum(pred_y[sensi.squeeze() == 1].squeeze() == 1) / s1\n",
    "    print(f\"#(S=0): {s0}, #(S=1): {s1}, #(y0): {y0}, #(y1): {y1}, P(y=1|s=0)={y1_s0:.3f}, P(y=1|s=1)={y1_s1:.3f}\")\n",
    "    return y1_s1 - y1_s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_time(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"Time: {duration:5.2f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens, dropout_prob = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(hiddens[:-1], hiddens[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(nn.Dropout(p=dropout_prob))\n",
    "        layers.pop()\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.optim = Adam(self.parameters())\n",
    "\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for param in self.parameters():\n",
    "            params.append(param.detach().cpu().flatten().numpy())\n",
    "        return np.hstack(params)\n",
    "\n",
    "    def forward(self, s_mb, x_mb, num_samples = 10):\n",
    "        pred = []\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            sx_mb = torch.cat([s_mb, x_mb], dim=1)\n",
    "            output = self.model(sx_mb)\n",
    "            pred.append(output.unsqueeze(0))  # Add the unsqueezed prediction\n",
    "        return  torch.mean(torch.cat(pred, dim=0), dim=0)\n",
    "\n",
    "    def predict(self, s_mb, x_mb):\n",
    "        probs = self(s_mb, x_mb)\n",
    "        pred_y = probs.detach().round().cpu().numpy()\n",
    "        return pred_y\n",
    "\n",
    "    def sample(self, s, x, scale=1.0):\n",
    "        prob = self(s, x)\n",
    "        y = torch.bernoulli(prob * scale)\n",
    "        return y.detach().cpu().numpy()\n",
    "\n",
    "    # @count_time\n",
    "    def fit(self, loader, valid_loader, save_path, device, max_epochs = 300, patience=20):\n",
    "        epoch, counter = 0, 0\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        while epoch < max_epochs:\n",
    "            loss = 0.\n",
    "            for s_mb, x_mb, y_mb in loader:\n",
    "                s_mb = s_mb.to(device)\n",
    "                x_mb = x_mb.to(device)\n",
    "                y_mb = y_mb.to(device)\n",
    "\n",
    "                batch_loss = 0.\n",
    "                for i in range(x_mb.size(1)):\n",
    "                    pred_y_mb = self(s_mb, x_mb[:, i])\n",
    "                    batch_loss += self.loss_fn(pred_y_mb, y_mb[:, i])\n",
    "                loss += batch_loss.item()\n",
    "\n",
    "                self.optim.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "            epoch += 1\n",
    "            valid_loss = self.eval(valid_loader, device)\n",
    "            if valid_loss <= best_loss:\n",
    "                # torch.save(self.state_dict(), save_path)\n",
    "                best_loss = valid_loss\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == patience:\n",
    "                    break\n",
    "            \n",
    "            if epoch == 1 or epoch % 100 == 0:\n",
    "                print(f'{epoch:6.0f} | loss: {loss:6.4f}')\n",
    "        print(f\"Classifier Fit Done in {epoch} epochs!, Counter: {counter}\")\n",
    "\n",
    "    def eval(self, loader, device, verbose=False):\n",
    "        loss = 0.\n",
    "        for s_mb, x_mb, y_mb in loader:\n",
    "            s_mb = s_mb.to(device)\n",
    "            x_mb = x_mb.to(device)\n",
    "            y_mb = y_mb.to(device)\n",
    "\n",
    "            batch_loss = 0.\n",
    "            for i in range(x_mb.size(1)):\n",
    "                s_mb = s_mb.to(device)\n",
    "                x_mb = x_mb.to(device)\n",
    "                y_mb = y_mb.to(device)\n",
    "\n",
    "                pred_y_mb = self(s_mb, x_mb[:, i].to(device))\n",
    "                batch_loss += self.loss_fn(pred_y_mb, y_mb[:, i])\n",
    "                loss += batch_loss.item()\n",
    "\n",
    "                if verbose:\n",
    "                    pred_y_mb = self.predict(s_mb, x_mb[:, i])\n",
    "                    true_y_mb = y_mb[:, i].cpu().numpy()\n",
    "                    s_mb_np = s_mb.cpu().numpy()\n",
    "\n",
    "                    acc = metrics.accuracy_score(true_y_mb, pred_y_mb) * 100\n",
    "                    fair = demographic_parity(s_mb_np, pred_y_mb)\n",
    "                    print(f\"Step: {i:6.0f}, ACC: {acc:6.2f}%, FAIR: {fair:6.2f}\\n\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveCLF(nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(x_dim, 1)\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.optim = Adam(self.parameters())\n",
    "\n",
    "    def forward(self, s, x):\n",
    "        return torch.round(torch.sigmoid(self.linear(x)))\n",
    "    \n",
    "    def prob(self, s, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "    \n",
    "    def fit(self, s, x, y, patience=10):\n",
    "        x = tensor(x)\n",
    "        y = tensor(y)\n",
    "\n",
    "        epoch, counter = 0, 0\n",
    "        best_loss = float('inf')\n",
    "        while True:\n",
    "            pred = self(s, x)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            \n",
    "            epoch += 1\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter == patience or epoch >= 5000:\n",
    "                    \n",
    "                    break\n",
    "#         print(f\"TrueModel Fit Done in {epoch} epochs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_loss(data, check_divergence = False):\n",
    "    pos_data = data[data[:, 0] == 1, 1:-1]\n",
    "    neg_data = data[data[:, 0] == 0, 1:-1]\n",
    "    w1 = torch.ones(len(pos_data)) / len(pos_data)\n",
    "    w2 = torch.ones(len(neg_data)) / len(neg_data)\n",
    "    M = ot.dist(pos_data, neg_data)\n",
    "    loss = sinkhorn_distance(pos_data, neg_data, check_divergence)\n",
    "    # loss = ot.sinkhorn2(w1, w2, M, 0.1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sinkhorn_distance(data1, data2, check_divergence, epsilon = 0.1, num_iters = 100, tol=1e-3):\n",
    "    n, m = data1.shape[0], data2.shape[0]\n",
    "    a = torch.ones(n) / n\n",
    "    b = torch.ones(m) / m\n",
    "    a.to(device)\n",
    "    b.to(device)\n",
    "\n",
    "    C = torch.cdist(data1, data2, p=2) ** 2\n",
    "    C = C / C.max()\n",
    "\n",
    "    K = torch.exp(-C / epsilon) \n",
    "    K.to(device)\n",
    "\n",
    "    u = torch.ones(n).to(device)\n",
    "    v = torch.ones(m).to(device)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        u_prev = u.clone()\n",
    "        v_prev = v.clone()\n",
    "        u = a.to(device) / (K.to(device) @ v.to(device) + 1e-8)\n",
    "        v = b.to(device) / (torch.transpose(K, 0, 1).to(device) @ u.to(device) + 1e-8)\n",
    "\n",
    "        if check_divergence:\n",
    "            u_change = torch.norm(u - u_prev)\n",
    "            v_change = torch.norm(v - v_prev)\n",
    "            if u_change < tol and v_change < tol:\n",
    "                print(f\"Converged in {i} iterations\")\n",
    "                break\n",
    "\n",
    "    plan = torch.diag(u) @ K @ torch.diag(v)\n",
    "    distance = torch.sum(plan * C).to(device)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gaussian(sample_size, seed=0):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    mu0, sigma0 = [-1, -1], [[5, 1], [1, 5]]\n",
    "    mu1, sigma1 = [1, 1], [[5, 1], [1, 5]]\n",
    "    s0 = np.ones(sample_size, dtype=float) * 0\n",
    "    X0 = np.random.multivariate_normal(mean=mu0, cov=sigma0, size=sample_size)\n",
    "    s1 = np.ones(sample_size, dtype=float) * 1\n",
    "    X1 = np.random.multivariate_normal(mean=mu1, cov=sigma1, size=sample_size)\n",
    "\n",
    "    s = np.concatenate((s0, s1))\n",
    "    X = np.vstack((X0, X1))\n",
    "    perm = list(range(int(2*sample_size)))\n",
    "    np.random.shuffle(perm)\n",
    "    s = s[perm]\n",
    "    X = X[perm]\n",
    "    y=np.random.binomial(1, 1/(1+np.exp(-(X[:,0]+2*X[:,1]-1))))\n",
    "\n",
    "    return s, X, y\n",
    "\n",
    "def sequential_data(s0, x0, y0, seq_len, l=0.5, noise_factor = 1, b = 0.5, seed=0, ground_truth = True):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    l_og = l\n",
    "    n = len(s0)\n",
    "    model = NaiveCLF(x0.shape[1])\n",
    "    x_dim = x0.shape[1]\n",
    "    hiddens = [x_dim + 1, 32, 64, 1]\n",
    "    true_model = TrueModel(hiddens)\n",
    "    true_model.fit(s0.reshape(-1,1), x0, y0.reshape(-1,1))\n",
    "    x0 = torch.from_numpy(x0).to(dtype=torch.float32)\n",
    "    x0.requires_grad = True\n",
    "    y0 = torch.from_numpy(y0).to(dtype=torch.float32)\n",
    "    model.fit(s0, x0, y0.view(n, 1))\n",
    "    theta_t = abs(model.optim.param_groups[0]['params'][-1].item())\n",
    "\n",
    "    \n",
    "    x = torch.empty(n, seq_len, x0.shape[1])\n",
    "    y= torch.empty(n, seq_len, 1)\n",
    "    x[:, 0, :] = x0\n",
    "    y[:, 0, :] = y0.view(n, 1)\n",
    "\n",
    "    prevy = y0\n",
    "    \n",
    "    for i in range(1, seq_len):\n",
    "        yhat = model(s0, x[:,i-1,:])\n",
    "        for j in range(n):\n",
    "            \n",
    "            if yhat[j] == 1 and prevy[j] == 0:\n",
    "                l = -l_og\n",
    "            elif yhat[j] == 1 and prevy[j] == 1:\n",
    "                l = l_og\n",
    "            else:\n",
    "                l = 0\n",
    "            for k in range(x0.shape[1]):\n",
    "                x[j, i, k] = np.random.randn()*noise_factor + x[j, i-1, k] + l*theta_t + s0[j]*b + (1-s0[j])*0.1\n",
    "        if ground_truth:\n",
    "            y[:, i, :] = torch.bernoulli((1 /(1+  torch.exp(-(x[:,i,0]+3*x[:,i,1]-1))))).view(n, 1)\n",
    "        else:\n",
    "            y[:, i, :] = tensor(true_model.sample(tensor(s0.reshape(-1,1)), x[:, i, :]))\n",
    "        prevy = y[:, i, :]\n",
    "        \n",
    "\n",
    "    # x = np.array(x, dtype=np.float32).reshape((n, seq_len, 2))\n",
    "    #y = np.array(y, dtype=np.int32).reshape(n, seq_len, 1)\n",
    "    return x, y, model\n",
    "\n",
    "s0, x0, y0 = gen_gaussian(int(n/2), seed=52)\n",
    "x, y, unfair_clf = sequential_data(s0, x0, y0, seq_len, l=epsilon, noise_factor=0.1, b=b, seed=52, ground_truth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_taiwan(file_path, n, seq_len, seed=0):\n",
    "\n",
    "    df = pd.read_excel(file_path, header=1)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    df = df[(df['PAY_AMT1'] < 10000) & (df['PAY_AMT1'] > 10)]\n",
    "    df = df[(df['PAY_AMT2'] < 10000) & (df['PAY_AMT2'] > 10)]\n",
    "    df = df[(df['PAY_AMT3'] < 10000) & (df['PAY_AMT3'] > 10)]\n",
    "\n",
    "    label0 = df[(df['SEX'] == 1)].sample(n=n, replace=False, random_state=seed)\n",
    "    label1 = df[(df['default payment next month'] == 1 ) & (df['SEX'] == 2)].sample(n=int(n*0.6), replace=False, random_state=seed)\n",
    "    label2 = df[(df['default payment next month'] == 0 ) & (df['SEX'] == 2)].sample(n=n-int(n*0.6), replace=False, random_state=seed)\n",
    "\n",
    "    df = pd.concat([label0, label1, label2], axis=0)\n",
    "    X = df.iloc[:, 18:20].apply(lambda x: 3 * (x - x.mean()) / (x.max() - x.min()))\n",
    "    Z = df.iloc[:, 1:2].apply(lambda x: 3 * (x - x.mean()) / (x.max() - x.min()))\n",
    "    S = df['SEX'] - 1\n",
    "    Y = df['default payment next month'].replace({0: 1, 1: 0})\n",
    "    data = pd.concat([S, Z, X, Y], axis=1)\n",
    "\n",
    "    data = data.rename(columns={'default payment next month': 'y'})\n",
    "    s0 = S.to_numpy()\n",
    "    x0 = pd.concat([Z, X], axis=1).to_numpy()\n",
    "    y0 = Y.to_numpy()\n",
    "    _, _, unfair_clf = sequential_data(s0, x0, y0, seq_len, l=0.1, noise_factor=0.01, b = 0.01, seed=seed, ground_truth=False)\n",
    "\n",
    "    return s0, x0, y0, unfair_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data')\n",
    "file_path = '/home/fagumuci/Long-Term-EI/Long-Term-Equal-Improvability/SimLoan/data/default of credit card clients.xls'\n",
    "df = pd.read_excel(file_path, header=1)\n",
    "df = df[(df['PAY_AMT1'] < 10000) & (df['PAY_AMT1'] > 10)]\n",
    "df = df[(df['PAY_AMT2'] < 10000) & (df['PAY_AMT2'] > 10)]\n",
    "df = df[(df['PAY_AMT3'] < 10000) & (df['PAY_AMT3'] > 10)]\n",
    "\n",
    "label0 = df[(df['default payment next month'] == 1) & (df['SEX'] == 1)].sample(n=int(n/4), replace=False, random_state=2021)\n",
    "label1 = df[(df['default payment next month'] == 0) & (df['SEX'] == 1)].sample(n=int(n/4), replace=False, random_state=2021)\n",
    "label2 = df[(df['default payment next month'] == 1) & (df['SEX'] == 2)].sample(n=int(n/4), replace=False, random_state=2021)\n",
    "label3 = df[(df['default payment next month'] == 0) & (df['SEX'] == 2)].sample(n=int(n/4), replace=False, random_state=2021)\n",
    "\n",
    "df = pd.concat([label0, label1, label2, label3], axis=0)\n",
    "X = df.iloc[:, 18:20].apply(lambda x: 3 * (x - x.mean()) / (x.max() - x.min()))\n",
    "Z = df.iloc[:, 1:2].apply(lambda x: 3 * (x - x.mean()) / (x.max() - x.min()))\n",
    "S = df['SEX'] - 1\n",
    "Y = df['default payment next month'].replace({0: 1, 1: 0})\n",
    "data = pd.concat([S, Z, X, Y], axis=1)\n",
    "\n",
    "data = data.rename(columns={'default payment next month': 'y'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data')\n",
    "file_path = '/home/fagumuci/Long-Term-EI/Long-Term-Equal-Improvability/SimLoan/data/default of credit card clients.xls'\n",
    "s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=52)\n",
    "x, y, unfair_clf = sequential_data(s0, x0, y0, seq_len, l=epsilon, noise_factor=0.01, b = b, seed=52, ground_truth=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.detach().numpy()\n",
    "y = y.detach().numpy()\n",
    "y = y.astype(int)\n",
    "s0 = np.array(s0).reshape(len(s0),1)\n",
    "s_train, s_test, x_train, x_test, y_train, y_test = train_test_split(s0, x, y, test_size=test_size, random_state=10)\n",
    "s_train, s_valid, x_train, x_valid, y_train, y_valid = train_test_split(s_train, x_train, y_train, test_size=valid_size, random_state=10)\n",
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.h0_linear = nn.Linear(in_size, hidden_size)\n",
    "        self.rnn = nn.GRU(in_size + 3, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, in_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x0, noise, s0, clf):\n",
    "        ss = torch.clone(s0)\n",
    "        ss = ss.to(x0.device)\n",
    "        s0 = torch.zeros(s0.size(0), 2).to(device).scatter_(1, s0.long(), torch.ones_like(s0))\n",
    "        s0 = s0.to(x0.device)\n",
    "\n",
    "        h0 = self.h0_linear(x0)\n",
    "        h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        yt = clf(ss, x0)\n",
    "        \n",
    "        xs, ys = [x0], [yt]\n",
    "        for i in range(noise.size(1)):\n",
    "            \n",
    "            y_noise = torch.cat([s0, yt, noise[:, i]], dim=-1).unsqueeze(1)\n",
    "            output, h0 = self.rnn(y_noise, h0)\n",
    "            # xt = self.sigmoid(self.linear(output).squeeze())\n",
    "            xt = self.linear(output).squeeze()\n",
    "            yt = clf(ss, xt)\n",
    "\n",
    "            xs.append(xt)\n",
    "            ys.append(yt)\n",
    "\n",
    "        xs = torch.stack(xs, dim=1)\n",
    "        ys = torch.stack(ys, dim=1)\n",
    "        return xs, ys, ys.round().detach()\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = nn.GRU(in_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hn = self.rnn(x)\n",
    "        output = self.linear(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "class DistributionDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddens):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        for in_dim, out_dim in zip(hiddens[:-1], hiddens[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "        layers.pop()\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(x_dim, g_hidden_size, g_num_layers)\n",
    "generator.to(device)\n",
    "discriminator = Discriminator(x_dim, d_hidden_size, d_num_layers)\n",
    "discriminator.to(device)\n",
    "clf = Classifier(c_hiddens, dropout_prob=0.2)\n",
    "clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, RMSprop\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def train_discriminator(clf, G, D, optim, loss_fn, xs, zs, ss):\n",
    "    xs_fake, _, _ = G(xs[:, 0], zs, ss, clf)\n",
    "    fake = D(xs_fake.detach())\n",
    "    loss_fake = loss_fn(fake, torch.zeros_like(fake))\n",
    "\n",
    "    real = D(xs)\n",
    "    loss_real = loss_fn(real, torch.ones_like(real))\n",
    "\n",
    "    loss = loss_fake + loss_real\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_moment_loss(x_pred, x_true):\n",
    "    m1 = torch.mean(torch.abs(x_pred.mean(dim=0) - x_true.mean(dim=0)))\n",
    "    m2 = torch.mean(torch.abs(\n",
    "        torch.sqrt(x_pred.var(dim=0, unbiased=False) + 1e-6) -\n",
    "        torch.sqrt(x_true.var(dim=0, unbiased=False) + 1e-6)\n",
    "    ))\n",
    "    return m1 + m2\n",
    "\n",
    "\n",
    "def train_generator(clf, G, D, optim, loss_fn, xs, zs, ss, gamma=100):\n",
    "    xs_fake, _, _ = G(xs[:, 0], zs, ss, clf)\n",
    "    fake = D(xs_fake)\n",
    "\n",
    "    loss1 = loss_fn(fake, torch.ones_like(fake))\n",
    "    loss2 = get_moment_loss(xs_fake, xs)\n",
    "    loss = loss1 + gamma * loss2\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss, loss2\n",
    "\n",
    "#@count_time\n",
    "def train_gan(loader, clf, G, D, n_epochs, device):\n",
    "    g_optim = Adam(G.parameters())\n",
    "    d_optim = Adam(D.parameters())\n",
    "    loss_fn = nn.BCELoss()\n",
    "    hist_mmt = np.empty(0)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i, (s_mb, x_mb, y_mb) in enumerate(loader, start=1):\n",
    "            batch, seq, dim = x_mb.size()\n",
    "            x_mb = x_mb.to(device)\n",
    "            z_mb = torch.rand(batch, seq-1, dim).to(device)\n",
    "            s_mb = s_mb.to(device)\n",
    "            y_mb = y_mb.to(device)\n",
    "\n",
    "            for _ in range(2):\n",
    "                g_loss, mmt_loss = train_generator(clf, G, D, g_optim, loss_fn, x_mb, z_mb, s_mb)\n",
    "                hist_mmt = np.append(hist_mmt, float(mmt_loss))\n",
    "\n",
    "            for _ in range(1):\n",
    "                d_loss = train_discriminator(clf, G, D, d_optim, loss_fn, x_mb, z_mb, s_mb)\n",
    "\n",
    "            step = epoch * len(loader) + i\n",
    "            if step % 1000 == 0:\n",
    "                print(f'Epoch: {epoch: 6.0f} | step: {step:6.0f} | d_loss: {d_loss:6.4f} | g_loss: {g_loss: 6.4f} | mmt_loss: {mmt_loss:6.4f}')\n",
    "\n",
    "    hist_mmt = np.asarray(hist_mmt)\n",
    "    plt.plot(np.arange(0, len(hist_mmt), 1), hist_mmt)\n",
    "    # plt.plot(np.arange(0, len(hist_mmt) + 1, 1), np.arange(0, len(hist_mmt) + 1, 1))\n",
    "\n",
    "def generate_dataset_from_gan(loader, clf, G, device, extra_seq=0):\n",
    "    gen_s, gen_x, gen_y = [], [], []\n",
    "\n",
    "    batch_size = None\n",
    "    for s_mb, x_mb, y_mb in loader:\n",
    "        batch, seq_len, x_dim = x_mb.shape\n",
    "        if batch_size is None:\n",
    "            batch_size = batch\n",
    "\n",
    "        x_mb = x_mb.to(device)\n",
    "        z_mb = torch.randn(batch, seq_len + extra_seq - 1, x_dim).to(device)\n",
    "\n",
    "        gen_x_mb, _, gen_y_mb = G(x_mb[:, 0], z_mb, s_mb, clf)\n",
    "        \n",
    "        gen_s.append(s_mb)\n",
    "        gen_x.append(gen_x_mb)\n",
    "        gen_y.append(gen_y_mb)\n",
    "\n",
    "    gen_s = torch.cat(gen_s, dim=0).detach().cpu().numpy()\n",
    "    gen_x = torch.cat(gen_x, dim=0).detach().cpu().numpy()\n",
    "    gen_y = torch.cat(gen_y, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    gen_data = TensorDataset(tensor(gen_s), tensor(gen_x), tensor(gen_y))\n",
    "    gen_loader = DataLoader(gen_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return gen_loader, gen_s, gen_x, gen_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tensor(s_train).to(device), tensor(x_train).to(device), tensor(y_train).to(device))\n",
    "valid_data = TensorDataset(tensor(s_valid).to(device), tensor(x_valid).to(device),tensor(y_valid).to(device))\n",
    "test_data = TensorDataset(tensor(s_test).to(device), tensor(x_test).to(device), tensor(y_test).to(device))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_data, batch_size=len(valid_data), shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.cuda()\n",
    "if clf_path.exists():\n",
    "    clf.load_state_dict(torch.load(clf_path, map_location=device))\n",
    "else:\n",
    "    clf.fit(train_loader, valid_loader, clf_path, device)\n",
    "    torch.save(clf.state_dict(), clf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.eval(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gan_path.exists():\n",
    "    generator.load_state_dict(torch.load(gan_path, map_location=device))\n",
    "else:\n",
    "    train_gan(train_loader, unfair_clf.to(device), generator, discriminator, gan_epochs, device)\n",
    "    torch.save(generator.state_dict(), gan_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train_loader, gen_train_s, gen_train_x, gen_train_y = generate_dataset_from_gan(train_loader, unfair_clf.to(device), generator, device)\n",
    "gen_valid_loader, gen_valid_s, gen_valid_x, gen_valid_y = generate_dataset_from_gan(valid_loader, unfair_clf.to(device), generator, device)\n",
    "gen_test_loader, gen_test_s, gen_test_x, gen_test_y = generate_dataset_from_gan(test_loader, unfair_clf.to(device), generator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.vstack([s_train, s_test, s_valid])\n",
    "x = np.vstack([x_train, x_test, x_valid])\n",
    "y = np.vstack([y_train, y_test, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.vstack([s_train, s_test, s_valid])\n",
    "x = np.vstack([x_train, x_test, x_valid])\n",
    "y = np.vstack([y_train, y_test, y_valid])\n",
    "gan_output = np.empty((0, x_train.shape[2] + 2))\n",
    "\n",
    "for i in range(seq_len):\n",
    "    temp = np.hstack([s, x[:, i, :], y[:, i, :]])\n",
    "    gan_output = np.vstack([gan_output, temp])\n",
    "\n",
    "df = pd.DataFrame(gan_output)\n",
    "df = df.set_axis(['SEX', 'LIMIT_BAL', 'PAY_AMT1', 'PAY_AMT2', 'Y'], axis=1)\n",
    "df.to_csv('taiwan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = np.empty((0, int(x_dim*2 + 3)))\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    temp_pair = np.empty((1, int(x_dim*2 + 3)))\n",
    "    temp_pair[0, 0] = s0[i]\n",
    "    for j in range(x.shape[1] - 1):\n",
    "        for k in range(x[0][0].shape[0]):\n",
    "            temp_pair[0, 1+k] = x[i, j, k]\n",
    "        temp_pair[0, 1+x_dim] = y[i, j][0]\n",
    "        for k in range(x[0][0].shape[0]):\n",
    "            temp_pair[0, 2 + x.shape[2] + k] = x[i, j+1, k]\n",
    "        temp_pair[0, - 1] = y[i, j+1][0]\n",
    "        pairs = np.vstack([pairs, temp_pair])\n",
    "        \n",
    "df = pd.DataFrame(pairs)\n",
    "df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "df.to_csv('taiwan.csv')   \n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets.toy import ToySCM\n",
    "from datasets.taiwan import TaiwanSCM\n",
    "from models.vaca import VACA\n",
    "import utils.args_parser  as argtools\n",
    "from data_modules.het_scm import HeterogeneousSCMDataModule\n",
    "\n",
    "scm = TaiwanSCM(None, 'train', len(pairs))\n",
    "model_file = os.path.join('_params', 'model_vaca.yaml')\n",
    "trainer_file = os.path.join('_params', 'trainer.yaml')\n",
    "\n",
    "cfg = argtools.parse_args(model_file)\n",
    "cfg.update(argtools.parse_args(trainer_file))\n",
    "\n",
    "cfg['dataset'] = {\n",
    "    'name': 'taiwan',\n",
    "    'params1': {},\n",
    "    'params2': {}\n",
    "}\n",
    "\n",
    "cfg['dataset']['params1']['batch_size'] = batch_size\n",
    "cfg['dataset']['params1']['num_samples_tr'] = len(pairs)\n",
    "cfg['dataset']['params1']['num_workers'] = 0\n",
    "cfg['dataset']['params1']['equations_type'] = 'non-linear'\n",
    "cfg['dataset']['params1']['normalize'] = 'lik'\n",
    "cfg['dataset']['params1']['lambda_'] = 0.05\n",
    "cfg['dataset']['params1']['data_dir'] = '../Data'\n",
    "cfg['dataset']['params1']['device'] = device\n",
    "cfg['dataset']['params1']['dataset_name'] = 'taiwan'\n",
    "\n",
    "dataset_params = cfg['dataset']['params1']\n",
    "data_module = HeterogeneousSCMDataModule(**dataset_params)\n",
    "\n",
    "data_module.train_dataset._create_data()\n",
    "data_module.valid_dataset._create_data()\n",
    "data_module.test_dataset._create_data()\n",
    "data_module.prepare_data()\n",
    "data_module.train_dataloader()\n",
    "data_module.test_dataloader()\n",
    "data_module.val_dataloader()\n",
    "\n",
    "\n",
    "cfg['model']['params']['is_heterogeneous'] = scm.is_heterogeneous\n",
    "cfg['model']['params']['likelihood_x'] = scm.likelihood_list\n",
    "\n",
    "cfg['model']['params']['num_nodes'] = scm.num_nodes\n",
    "cfg['model']['params']['edge_dim'] = scm.edge_dimension\n",
    "\n",
    "model_params = cfg['model']['params']\n",
    "\n",
    "model_vaca = VACA(**model_params)\n",
    "model_vaca.set_random_train_sampler(data_module.get_random_train_sampler())\n",
    "\n",
    "model_vaca.set_optim_params(optim_params=cfg['optimizer'],\n",
    "                            sched_params=cfg['scheduler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models._evaluator import MyEvaluator\n",
    "\n",
    "evaluator = MyEvaluator(model=model_vaca,\n",
    "                        intervention_list=data_module.train_dataset.get_intervention_list(),\n",
    "                        scaler=data_module.scaler\n",
    "                        )\n",
    "model_vaca.set_my_evaluator(evaluator=evaluator)\n",
    "\n",
    "assert evaluator is not None\n",
    "is_training = train_vaca\n",
    "del cfg['trainer']['progress_bar_refresh_rate']\n",
    "del cfg['trainer']['flush_logs_every_n_steps']\n",
    "del cfg['trainer']['terminate_on_nan']\n",
    "del cfg['trainer']['auto_select_gpus']\n",
    "del cfg['trainer']['weights_summary']\n",
    "cfg['trainer']['enable_model_summary'] = False\n",
    "del cfg['trainer']['gpus']\n",
    "del cfg['trainer']['track_grad_norm']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "yaml_file = ''\n",
    "if yaml_file == '':\n",
    "        save_dir = argtools.mkdir(os.path.join(cfg['root_dir'],\n",
    "                                               argtools.get_experiment_folder(cfg),\n",
    "                                               str(cfg['seed'])))\n",
    "else:\n",
    "        save_dir = os.path.join(yaml_file.split('/')[:-1])\n",
    "print(f'Save dir: {save_dir}')\n",
    "# trainer = pl.Trainer(**cfg['model'])\n",
    "logger = TensorBoardLogger(save_dir=save_dir, name='logs', default_hp_metric=False)\n",
    "out = logger.log_hyperparams(argtools.flatten_cfg(cfg))\n",
    "\n",
    "save_dir_ckpt = argtools.mkdir(os.path.join(save_dir, 'ckpt'))\n",
    "ckpt_file = argtools.newest(save_dir_ckpt)\n",
    "callbacks = []\n",
    "if is_training == 1:\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(monitor=model_vaca.monitor(),\n",
    "                                     mode=model_vaca.monitor_mode(),\n",
    "                                     save_top_k=1,\n",
    "                                     save_last=True,\n",
    "                                     filename='checkpoint-{epoch:02d}',\n",
    "                                     dirpath=save_dir_ckpt)\n",
    "\n",
    "    callbacks = [checkpoint]\n",
    "\n",
    "    if cfg['early_stopping']:\n",
    "            early_stopping = EarlyStopping(model_vaca.monitor(), mode=model_vaca.monitor_mode(), min_delta=0.0,\n",
    "                                           patience=50)\n",
    "            callbacks.append(early_stopping)\n",
    "\n",
    "    if ckpt_file is not None:\n",
    "            print(f'Loading model training: {ckpt_file}')\n",
    "            trainer = pl.Trainer(logger=logger, callbacks=callbacks, **cfg['trainer'], devices='auto', accelerator='cpu')\n",
    "    else:\n",
    "\n",
    "            trainer = pl.Trainer(logger=logger, callbacks=callbacks, **cfg['trainer'], devices='auto', accelerator='cpu')\n",
    "\n",
    "    trainer.fit(model=model_vaca, datamodule=data_module)\n",
    "    trainer.validate(ckpt_path=ckpt_file, dataloaders=data_module.val_dataloader())\n",
    "    trainer.test(ckpt_path=ckpt_file, dataloaders=data_module.test_dataloader())\n",
    "    argtools.save_yaml(cfg, file_path=os.path.join(save_dir, 'hparams_full.yaml'))\n",
    "    \n",
    "else:\n",
    "        model_vaca = VACA.load_from_checkpoint(ckpt_file, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator.set_model(model_vaca)\n",
    "model_vaca.set_my_evaluator(evaluator=evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model_vaca.parameters())\n",
    "params = int(sum([np.prod(p.size()) for p in model_parameters]))\n",
    "model_vaca.eval()\n",
    "model_vaca.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_to_csv(s, x, y, dataset):\n",
    "    if isinstance(s, torch.Tensor):\n",
    "        s = s.cpu().detach().numpy()\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.cpu().detach().numpy()\n",
    "    if isinstance(y, torch.Tensor):\n",
    "        y = y.cpu().detach().numpy()\n",
    "    \n",
    "    if dataset == 'toy':\n",
    "        pairs = np.empty((0, 7))\n",
    "        for i in range(x.shape[0]):\n",
    "            temp = np.empty((1, x.shape[2]*2 + 3))\n",
    "            temp[0, 0] = s[i]\n",
    "            for j in range(x.shape[1] - 1):\n",
    "                for k in range(x.shape[2]):\n",
    "                    temp[0, 1+k] = x[i, j, k]\n",
    "                temp[0, 1+x.shape[2]] = y[i, j][0]\n",
    "                for k in range(x.shape[2]):\n",
    "                    temp[0, 2 + x.shape[2] + k] = x[i, j+1, k]\n",
    "                temp[0, -1] = y[i, j+1][0]\n",
    "            pairs = np.vstack([pairs, temp])\n",
    "        df = pd.DataFrame(pairs)\n",
    "        df = df.set_axis(['s', 'x1', 'z1', 'y1', 'x2', 'z2', 'y2'], axis=1)\n",
    "        df.to_csv('steps.csv')\n",
    "        return pairs\n",
    "    elif dataset == 'taiwan':\n",
    "        pairs = np.empty((0, 9))\n",
    "        for i in range(x.shape[0]):\n",
    "            temp = np.empty((1, x.shape[2]*2 + 3))\n",
    "            temp[0, 0] = s[i]\n",
    "            for j in range(x.shape[1] - 1):\n",
    "                for k in range(x.shape[2]):\n",
    "                    temp[0, 1+k] = x[i, j, k]\n",
    "                temp[0, 1+x.shape[2]] = y[i, j][0]\n",
    "                for k in range(x.shape[2]):\n",
    "                    temp[0, 2 + x.shape[2] + k] = x[i, j+1, k]\n",
    "                temp[0, -1] = y[i, j+1][0]\n",
    "            pairs = np.vstack([pairs, temp])\n",
    "\n",
    "        df = pd.DataFrame(pairs)\n",
    "        df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "        df.to_csv('taiwan.csv')    \n",
    "        return pairs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaca_dataset(batch_size, pairs, device, dataset):\n",
    "    model_file = os.path.join('_params', 'model_vaca.yaml')\n",
    "\n",
    "    cfg = argtools.parse_args(model_file)    \n",
    "    cfg['dataset'] = {\n",
    "        'name': dataset,\n",
    "        'params1': {},\n",
    "        'params2': {}\n",
    "    }\n",
    "\n",
    "    cfg['dataset']['params1']['batch_size'] = batch_size\n",
    "    cfg['dataset']['params1']['num_samples_tr'] = len(pairs)\n",
    "    cfg['dataset']['params1']['num_workers'] = 0\n",
    "    cfg['dataset']['params1']['equations_type'] = 'non-linear'\n",
    "    cfg['dataset']['params1']['normalize'] = 'lik'\n",
    "    cfg['dataset']['params1']['lambda_'] = 0.05\n",
    "    cfg['dataset']['params1']['data_dir'] = '../Data'\n",
    "    cfg['dataset']['params1']['device'] = device\n",
    "    cfg['dataset']['params1']['dataset_name'] = dataset\n",
    "\n",
    "    dataset_params = cfg['dataset']['params1']\n",
    "    data_module = HeterogeneousSCMDataModule(**dataset_params)\n",
    "\n",
    "    data_module.train_dataset._create_data()\n",
    "    data_module.valid_dataset._create_data()\n",
    "    data_module.test_dataset._create_data()\n",
    "    data_module.total_dataset._create_data()\n",
    "    data_module.prepare_data()\n",
    "    data_module.train_dataloader()\n",
    "    data_module.test_dataloader()\n",
    "    data_module.val_dataloader()\n",
    "    data_module.total_dataloader()\n",
    "    return data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intervention(nodes, values):\n",
    "    inter = {}\n",
    "    for i in range(len(nodes)):\n",
    "        inter[nodes[i]] = values[i].item()\n",
    "    return inter\n",
    "\n",
    "def logistic(x):\n",
    "    return torch.log(1 + torch.exp(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.h0_linear = nn.Linear(in_size, hidden_size)\n",
    "        self.rnn = nn.GRU(in_size + 3, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, in_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x0, noise, s0, clf):\n",
    "        ss = torch.clone(s0)\n",
    "        ss = ss.to(x0.device)\n",
    "        s0 = torch.zeros(s0.size(0), 2).to(x0.device).scatter_(1, s0.long(), torch.ones_like(s0, device=x0.device))\n",
    "        s0 = s0.to(x0.device)\n",
    "\n",
    "        h0 = self.h0_linear(x0)\n",
    "        h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        yt = clf(ss, x0)\n",
    "        \n",
    "        xs, ys = [x0], [yt]\n",
    "        for i in range(noise.size(1)):\n",
    "            \n",
    "            y_noise = torch.cat([s0, yt, noise[:, i]], dim=-1).unsqueeze(1)\n",
    "            output, h0 = self.rnn(y_noise, h0)\n",
    "            # xt = self.sigmoid(self.linear(output).squeeze())\n",
    "            xt = self.linear(output).squeeze()\n",
    "            yt = clf(ss, xt)\n",
    "\n",
    "            xs.append(xt)\n",
    "            ys.append(yt)\n",
    "\n",
    "        xs = torch.stack(xs, dim=1)\n",
    "        ys = torch.stack(ys, dim=1)\n",
    "        return xs, ys, ys.round()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator2(x_dim, g_hidden_size, g_num_layers)\n",
    "generator.to(device)\n",
    "generator.load_state_dict(torch.load(gan_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervention_step(loader, vaca, intervention_nodes, e, nodes):\n",
    "    k = 0\n",
    "    x_gan_input_list = []\n",
    "\n",
    "    for batch in loader:\n",
    "\n",
    "            batch.x_i = batch.x + e[k].to(batch.x.device).view(batch.x.shape)\n",
    "            \n",
    "        \n",
    "            z_f, _ = vaca.model.encoder(batch.x, batch.edge_index, edge_attr=batch.edge_attr, return_mean=True, node_ids=batch.node_ids)\n",
    "            z_cf_I, _ = vaca.model.encoder(batch.x_i, batch.edge_index_i, edge_attr=batch.edge_attr_i,return_mean=True, node_ids=batch.node_ids)\n",
    "\n",
    "            z_factual = z_f.reshape(1, -1)\n",
    "            z_cf_I = z_cf_I.reshape(1, -1)\n",
    "\n",
    "            z_dec = z_factual\n",
    "            z_dim = vaca.z_dim\n",
    "\n",
    "            for node_name in intervention_nodes:\n",
    "                node_idx = loader.dataset.nodes_list.index(node_name)\n",
    "                z_dec[:, z_dim * node_idx:z_dim * (node_idx + 1)] = z_cf_I[:, z_dim * node_idx: z_dim * (node_idx + 1)]\n",
    "\n",
    "            z_dec = z_dec.reshape(-1, z_dim)\n",
    "\n",
    "            x_CF, px_z = vaca.model.decoder(z_dec, batch.edge_index_i, edge_attr=batch.edge_attr_i, return_type='mean',  node_ids=batch.node_ids)\n",
    "            adj = []\n",
    "            for i in range(max(x_CF.shape)):\n",
    "                if type(px_z.distributions[i]) == torch.distributions.normal.Normal:\n",
    "                    adj.append(px_z.distributions[i].stddev)\n",
    "                else:\n",
    "                    adj.append(torch.tensor(0))\n",
    "\n",
    "            x_CF+=(torch.tensor(adj)*torch.randn_like(x_CF))\n",
    "\n",
    "            # x_CF = x_CF.view(len(nodes), -1)\n",
    "\n",
    "            x_gan_input_list.append(x_CF[:,-int((len(nodes))/2):-1].clone())\n",
    "\n",
    "            k+=1\n",
    "            if k == len(loader):\n",
    "                break\n",
    "\n",
    "    x_gan_input = torch.stack(x_gan_input_list, dim=0)\n",
    "\n",
    "\n",
    "    return x_gan_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene(datamodule, loader, vaca, generator, clf, efforts, seq_len, dataset, device):\n",
    "    nodes = datamodule.train_dataset.nodes_list\n",
    "    intervention_nodes = datamodule.train_dataset.nodes_to_intervene\n",
    "    # _inter = intervention(intervention_nodes, efforts[0])\n",
    "    s = tensor(loader.dataset.X[:,0]).view(len(loader.dataset), 1).to(device)\n",
    "    x_gan_input = torch.empty(len(loader), int((len(nodes)-2)))\n",
    "    y_pred = torch.empty(len(loader), 0).to(device)\n",
    "    y_pred.requires_grad_(True)\n",
    "    datamodule.batch_size = 1\n",
    "    x_post_int = torch.empty(len(loader), seq_len, int(len(nodes)/2-1)).to(device)\n",
    "    x_post_int[:, 0, :] = tensor(loader.dataset.X[:, 1:int(len(nodes)/2)]).clone().to(device)\n",
    "    inter_index = []\n",
    "    for n in range(len(nodes)):\n",
    "        if nodes[n] in intervention_nodes:\n",
    "            inter_index.append(n)\n",
    "\n",
    "    t = torch.empty(len(loader), 0)\n",
    "    for ns in inter_index:\n",
    "        t = torch.cat([t, torch.ones((len(loader),1))*ns], dim=1)\n",
    "\n",
    "    t = t.type(torch.int64)\n",
    "\n",
    "    t = t.to(device)\n",
    "    efforts = efforts.to(device)\n",
    "    \n",
    "    for i in range(0, seq_len-1):\n",
    "        \n",
    "        e = torch.zeros((len(loader), len(nodes))).to(device).scatter(1, t, efforts.view(len(loader), seq_len-1, len(intervention_nodes))[:, i, :]).to(device)\n",
    "\n",
    "        datamodule.batch_size = 1\n",
    "\n",
    "        x_gan_input = intervention_step(loader, vaca, intervention_nodes, e, nodes)\n",
    "\n",
    "        z_mb = torch.randn(len(loader), 1, int((len(nodes)/2-1)))\n",
    "\n",
    "        gen_x_mb, _, gen_y_mb = generator(x_gan_input.view(len(loader), int((len(nodes)/2-1))).to(device), z_mb.to(device), s.reshape(len(loader),1).to(device), clf.to(device))\n",
    "        y_pred = torch.cat([y_pred, gen_y_mb[:,0].to(device)], 1)\n",
    "        \n",
    "        x_next_steps = gen_x_mb[:, -2:, :]\n",
    "        y_next_steps = gen_y_mb[:, -2:, :]\n",
    "        x_post_int[:, i, :] = gen_x_mb[:, 0, :].clone()\n",
    "\n",
    "        data = torch.cat([s.to(device), x_post_int[:, i, :].to(device), gen_y_mb[:,0].to(device)], 1)\n",
    "        \n",
    "        p_to_csv(s, x_next_steps, y_next_steps, dataset)\n",
    "\n",
    "        datamodule = vaca_dataset(1, y_next_steps, device, dataset)\n",
    "        loader = datamodule.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "    \n",
    "    x_post_int[:, 0, :] = tensor(loader.dataset.X[:, 1:int(len(nodes)/2)]).clone().to(device)\n",
    "\n",
    "    return y_pred, x_post_int, data\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effortNN(nn.Module):\n",
    "    def __init__(self, datamodule, loader, seq_len, device):\n",
    "        super(effortNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(int(datamodule.num_nodes), 128, bias=True, device=device)\n",
    "        self.ln1 = nn.LayerNorm(128)\n",
    "        self.fc2 = nn.Linear(128, 64, bias=True, device=device)\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "        self.fc3 = nn.Linear(64, int(len(datamodule.train_dataset.nodes_to_intervene) * int(seq_len-1)), device=device)\n",
    "        layers = []\n",
    "        layers.append(self.fc1)\n",
    "        layers.append(self.ln1)\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(self.fc2)\n",
    "        layers.append(self.ln2)\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(self.fc3)\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        efforts = self.fc3(x)\n",
    "\n",
    "        return efforts\n",
    "    \n",
    "\n",
    "def budget_penalty(efforts, total_budget):\n",
    "    b_loss = 0\n",
    "    for i in range(efforts.shape[0]):\n",
    "        b_loss += torch.relu(torch.sum(torch.abs(efforts[i])) - total_budget)\n",
    "    return b_loss\n",
    "\n",
    "def total_loss(efforts, total_budget, improvement_loss):\n",
    "    budget_loss = budget_penalty(efforts, total_budget)\n",
    "    t_loss = 100000*budget_loss + improvement_loss\n",
    "    return t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_st_loss(s, y_pred):\n",
    "    st_loss = 0.0\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_neg = y_pred[s.squeeze() == 0, i]\n",
    "        y_pos = y_pred[s.squeeze() == 1, i]\n",
    "        p_y_pos = torch.sum(y_pos >= 0.5) / len(y_pos)\n",
    "        p_y_neg = torch.sum(y_neg >= 0.5) / len(y_neg)\n",
    "        st_loss += torch.abs(p_y_pos - p_y_neg)\n",
    "\n",
    "    return st_loss/y_pred.shape[1]\n",
    "\n",
    "def compute_post_long_cond_probs(s, Xs, Ys, clf):\n",
    "    probs = {}\n",
    "    \n",
    "    for i in range(len(Xs)-1):\n",
    "        \n",
    "        XXs_comb = np.c_[s[s == 1], Xs[i][s == 1], Xs[i+1][s == 1]]\n",
    "        Xs_comb = np.c_[s[s == 1], Xs[i][s == 1]]\n",
    "        lr_up = LogisticRegression(random_state=2021).fit(XXs_comb, Ys[i][s == 1])\n",
    "        lr_dn = LogisticRegression(random_state=2021).fit(Xs_comb, Ys[i][s == 1])\n",
    "        probs_up = lr_up.predict_proba(XXs_comb)\n",
    "        probs_dn = lr_dn.predict_proba(Xs_comb)\n",
    "        probs[f'pos(y{i+1}=0)'] = probs_up[:, 0] / probs_dn[:, 0]\n",
    "        probs[f'pos(y{i+1}=1)'] = probs_up[:, 1] / probs_dn[:, 1]\n",
    "\n",
    "\n",
    "        XXs_comb = np.c_[s[s == 0], Xs[i][s == 0], Xs[i+1][s == 0]]\n",
    "        Xs_comb = np.c_[s[s == 0], Xs[i][s == 0]]\n",
    "        lr_up = LogisticRegression(random_state=2021).fit(XXs_comb, Ys[i][s == 0])\n",
    "        lr_dn = LogisticRegression(random_state=2021).fit(Xs_comb, Ys[i][s == 0])\n",
    "        probs_up = lr_up.predict_proba(XXs_comb)\n",
    "        probs_dn = lr_dn.predict_proba(Xs_comb)\n",
    "        probs[f'neg(y{i+1}=0)'] = probs_up[:, 0] / probs_dn[:, 0]\n",
    "        probs[f'neg(y{i+1}=1)'] = probs_up[:, 1] / probs_dn[:, 1]\n",
    "    return probs\n",
    "\n",
    "def compute_lt_loss(s, y_pred, y0):\n",
    "\n",
    "    y_neg = y_pred[(s.squeeze() == 0) & (y0.squeeze() == 0), -1]\n",
    "    y_pos = y_pred[(s.squeeze() == 1) & (y0.squeeze() == 0), -1]\n",
    "    p_y_neg = torch.sum(y_neg >= 0.5) / len(y_neg)\n",
    "    p_y_pos = torch.sum(y_pos >= 0.5) / len(y_pos)\n",
    "    lt_loss = torch.abs(p_y_pos - p_y_neg)\n",
    "    \n",
    "    return lt_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_to_csv(s, x[:, 0:2, :], y[:, 0:2, :], 'taiwan')\n",
    "data_module = vaca_dataset(batch_size, s, device, dataset='taiwan')\n",
    "original_pairs = data_module.total_dataloader().dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lstm_out: Tensor of shape (batch_size, seq_len, hidden_size)\n",
    "        Returns:\n",
    "            context: Tensor of shape (batch_size, hidden_size), the weighted sum of LSTM outputs\n",
    "            attention_scores: Tensor of shape (batch_size, seq_len), attention weights for each timestep\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = self.attention_weights(lstm_out).squeeze(-1)  # Shape: (batch_size, seq_len)\n",
    "        scores = torch.softmax(scores, dim=1)  # Normalize scores across timesteps\n",
    "\n",
    "        # Compute context vector as weighted sum of LSTM outputs\n",
    "        context = torch.sum(lstm_out * scores.unsqueeze(-1), dim=1)  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        return context, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effortLSTM(nn.Module):\n",
    "    def __init__(self, datamodule, loader, seq_len, device):\n",
    "        super(effortLSTM, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.num_interventions = len(datamodule.train_dataset.nodes_to_intervene)\n",
    "        self.hidden_size = 128\n",
    "\n",
    "        self.lstm = nn.LSTM(int((datamodule.num_nodes) + self.num_interventions), self.hidden_size, num_layers=2, batch_first=True)\n",
    "        self.attention = TemporalAttention(self.hidden_size)\n",
    "        self.init_interventions = nn.Parameter(torch.zeros(1, self.num_interventions))\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_interventions)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        h_0 = torch.zeros(2, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        efforts_seq = []\n",
    "        efforts = self.init_interventions.repeat(batch_size, 1)\n",
    "\n",
    "        all_hidden_states = []\n",
    "        x_squeezed = x.squeeze(1)\n",
    "        \n",
    "        for _ in range(seq_len-1):\n",
    "            x_combined = torch.cat([x_squeezed, efforts], dim=-1)\n",
    "            lstm_out, (h_0, c_0) = self.lstm(x_combined.unsqueeze(1), (h_0, c_0))\n",
    "            all_hidden_states.append(lstm_out.squeeze(1))\n",
    "\n",
    "            hidden_tensor = torch.stack(all_hidden_states, dim=1)\n",
    "            context, attention_scores = self.attention(hidden_tensor)\n",
    "\n",
    "            efforts = torch.nn.functional.relu(self.fc(context))\n",
    "            efforts_seq.append(efforts)\n",
    "\n",
    "        efforts_seq = torch.stack(efforts_seq, dim=1)\n",
    "\n",
    "        return efforts_seq\n",
    "    \n",
    "class effortLSTM_clf(nn.Module):\n",
    "    def __init__(self, datamodule, seq_len, clf, device):\n",
    "        super(effortLSTM_clf, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.num_interventions = len(datamodule.train_dataset.nodes_to_intervene)\n",
    "        self.hidden_size = 128\n",
    "        self.clf = clf\n",
    "\n",
    "        for param in self.clf.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(int((datamodule.num_nodes) + self.num_interventions), self.hidden_size, num_layers=2, batch_first=True)\n",
    "        self.attention = TemporalAttention(self.hidden_size)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_interventions)\n",
    "        self.init_interventions = nn.Parameter(torch.zeros(1, self.num_interventions))\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        s0 = x[:, 0, 0].view(x.size(0), 1)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        clf_outputs = self.clf(s0, x[:, 0, -4:-1])\n",
    "        \n",
    "        x_with_y = torch.cat([x[:,0,:-1], clf_outputs.view(batch_size,1)], dim=1)\n",
    "        h_0 = torch.zeros(2, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(2, batch_size, self.hidden_size).to(x.device)\n",
    "\n",
    "        efforts = self.init_interventions.repeat(batch_size, 1)\n",
    "\n",
    "        all_hidden_states = []\n",
    "\n",
    "        efforts_seq = []\n",
    "        \n",
    "        for _ in range(seq_len-1):\n",
    "            clf_outputs = self.clf(s0, x[:, 0, -4:-1])\n",
    "            x_with_y = torch.cat([x[:,0,:-1], clf_outputs.view(batch_size,1), efforts], dim=1).unsqueeze(1)\n",
    "            lstm_out, (h_0, c_0) = self.lstm(x_with_y, (h_0, c_0))\n",
    "\n",
    "            all_hidden_states.append(lstm_out.squeeze(1))\n",
    "            hidden_tensor = torch.stack(all_hidden_states, dim=1)\n",
    "            context, attention_scores = self.attention(hidden_tensor)\n",
    "\n",
    "            efforts = torch.nn.functional.relu(self.fc(context))\n",
    "            efforts_seq.append(efforts)\n",
    "\n",
    "        efforts_seq = torch.stack(efforts_seq, dim=1)\n",
    "\n",
    "        return efforts_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, best_loss, filepath):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_loss': best_loss\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_loss = checkpoint['best_loss']\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded - Resuming from epoch {epoch}, Best Loss: {best_loss}\")\n",
    "    return epoch, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_config(config_path=\"weights.yaml\"):\n",
    "    \"\"\"Loads the YAML configuration file.\"\"\"\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "def get_loss_weights(model, dataset, budget, config):\n",
    "    \"\"\"Retrieves the correct loss weights based on model, budget, and dataset.\"\"\"\n",
    "    try:\n",
    "        return config[\"weights\"][model][dataset][budget]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Invalid combination: model={model}, dataset={dataset}, budget={budget}\")\n",
    "\n",
    "# Example Usage\n",
    "config = load_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(t0)\n",
    "df = df.set_axis(['SEX', 'LIMIT_BAL', 'PAY_AMT1', 'PAY_AMT2', 'Y'], axis=1)\n",
    "df.to_csv('taiwan.csv')\n",
    "data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "original_pairs = data_module.total_dataloader().dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = original_pairs\n",
    "df = pd.DataFrame(t0)\n",
    "df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "df.to_csv('taiwan.csv')   \n",
    "data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "loader = data_module.train_dataloader()\n",
    "data_module.batch_size = 1\n",
    "loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "loader = data_module.train_dataloader()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "net = effortLSTM(data_module, loader, seq_len, device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "s = tensor(loader.dataset.X[:,0]).to(device)\n",
    "y0 = tensor(loader.dataset.X[:,x_dim+1]).to(device)\n",
    "\n",
    "b_list = [1, 2, 3, 4]\n",
    "config = load_config()\n",
    "for b in b_list:\n",
    "    budget = b*seq_len\n",
    "    df = pd.DataFrame(t0)\n",
    "    df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "    df.to_csv('taiwan.csv')   \n",
    "    loss_weights = get_loss_weights('LSTM', 'taiwan', budget, config)\n",
    "    loss_weights = [x/sum(loss_weights) for x in loss_weights]\n",
    "    imp_w, lt_w, st_w = loss_weights\n",
    "    data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "    loader = data_module.train_dataloader()\n",
    "    data_module.batch_size = 1\n",
    "    loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    net = effortLSTM(data_module, loader, seq_len, device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    efforts_path = log_dir / (f\"LSTM_effort_model\" + dataset + \"_budget_\" + str(budget) + \"_eps_\" + str(eps) + \"_b_\" + str(bet) + \".pth\")\n",
    "    checkpoint_path = log_dir / (f\"LSTM_effort_model\" + dataset + \"_budget_\" + str(budget) + \"_eps_\" + str(eps) + \"_b_\" + str(bet) + \"_checkpoint.pth\")\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    last_epoch=0\n",
    "   \n",
    "\n",
    "    hist_loss = []\n",
    "    for epoch in range(last_epoch, 100):\n",
    "        epoch_start = time.time()\n",
    "        net.train()\n",
    "        efforts = net(tensor(loader.dataset.X).view(len(loader.dataset), 1, int(x_dim*2)+3).to(device))\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, x_hat, data = intervene(data_module, loader, model_vaca, generator, clf, efforts.view(len(loader), seq_len-1, 2), seq_len, dataset ,device)\n",
    "        improvement_loss = loss_fn(y_hat, torch.ones_like(y_hat))\n",
    "        lt_fairness = compute_distance_loss(data)\n",
    "        st_fairness = compute_st_loss(s, y_hat)\n",
    "        t_loss = 10000*budget_penalty(efforts, budget*0.95) + imp_w*improvement_loss + lt_w*lt_fairness + st_w*st_fairness + 20 * torch.relu(compute_st_loss(s, y_hat[:, -1].unsqueeze(1)) - 0.05)\n",
    "        \n",
    "        t_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            df = pd.DataFrame(t0)\n",
    "            df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "            df.to_csv('taiwan.csv')   \n",
    "            data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "            val_loader = data_module.val_dataloader()\n",
    "            data_module.batch_size = 1\n",
    "            val_s = tensor(val_loader.dataset.X[:,0]).view(len(val_loader.dataset), 1).to(device)\n",
    "            val_loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "            val_efforts = net(tensor(val_loader.dataset.X).view(len(val_loader.dataset), 1, int(x_dim*2)+3).to(device))\n",
    "            val_y, val_x, val_data = intervene(data_module, val_loader, model_vaca, generator, clf, val_efforts, seq_len, 'taiwan', device)\n",
    "            val_loss = 10000*budget_penalty(val_efforts, budget*0.95) + imp_w*loss_fn(val_y, torch.ones_like(val_y)) + lt_w*compute_distance_loss(val_data) + st_w*compute_st_loss(val_s, val_y) + 20 * torch.relu(compute_st_loss(val_s, val_y[:, -1].unsqueeze(1)) - 0.05)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        df = pd.DataFrame(t0)\n",
    "        df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "        df.to_csv('taiwan.csv')   \n",
    "        epoch_end = time.time()\n",
    "        hist_loss.append(t_loss.item())\n",
    "        if (val_loss < best_loss and epoch > 0):\n",
    "            best_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save(net.state_dict(), efforts_path)\n",
    "            save_checkpoint(epoch, net, optimizer, best_loss, checkpoint_path)\n",
    "            e = epoch\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            patience += 1\n",
    "            if (patience > 10 and 10000*budget_penalty(efforts, budget*.95) < 0.01):\n",
    "                print('Early stopping')\n",
    "                print(f' Last save: {e}')\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch: 4.0f} | Total Loss: {t_loss: 8.4f} | Budget Loss: {10000*budget_penalty(efforts, budget*.95) : 3.2f} | Long Fairness Loss: {lt_fairness: 3.4f} | Improvement Loss: {improvement_loss: 3.4f} | Time: {epoch_end - epoch_start: 3.1f} | Patience: {patience}')\n",
    "\n",
    "    print('Last saved epoch: ', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = original_pairs\n",
    "df = pd.DataFrame(t0)\n",
    "df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "df.to_csv('taiwan.csv')   \n",
    "data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "loader = data_module.train_dataloader()\n",
    "data_module.batch_size = 1\n",
    "loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "loader = data_module.train_dataloader()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "net = effortNN(data_module, loader, seq_len, device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "s = tensor(loader.dataset.X[:,0]).to(device)\n",
    "y0 = tensor(loader.dataset.X[:,x_dim+1]).to(device)\n",
    "\n",
    "b_list = [1, 2, 3, 4]\n",
    "config = load_config()\n",
    "for b in b_list:\n",
    "    budget = b*seq_len\n",
    "    df = pd.DataFrame(t0)\n",
    "    df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "    df.to_csv('taiwan.csv')   \n",
    "    loss_weights = get_loss_weights('LSTM', 'taiwan', budget, config)\n",
    "    loss_weights = [x/sum(loss_weights) for x in loss_weights]\n",
    "    imp_w, lt_w, st_w = loss_weights\n",
    "    data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "    loader = data_module.train_dataloader()\n",
    "    data_module.batch_size = 1\n",
    "    loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    net = effortNN(data_module, loader, seq_len, device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    efforts_path = log_dir / (f\"NN_effort_model\" + dataset + \"_budget_\" + str(budget) + \"_eps_\" + str(eps) + \"_b_\" + str(bet) + \".pth\")\n",
    "    checkpoint_path = log_dir / (f\"NN_effort_model\" + dataset + \"_budget_\" + str(budget) + \"_eps_\" + str(eps) + \"_b_\" + str(bet) + \"_checkpoint.pth\")\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    last_epoch=0\n",
    "   \n",
    "\n",
    "    hist_loss = []\n",
    "    for epoch in range(last_epoch, 100):\n",
    "        epoch_start = time.time()\n",
    "        net.train()\n",
    "        efforts = net(tensor(loader.dataset.X).view(len(loader.dataset), 1, int(x_dim*2)+3).to(device))\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, x_hat, data = intervene(data_module, loader, model_vaca, generator, clf, efforts.view(len(loader), seq_len-1, 2), seq_len, dataset ,device)\n",
    "        improvement_loss = loss_fn(y_hat, torch.ones_like(y_hat))\n",
    "        lt_fairness = compute_distance_loss(data)\n",
    "        st_fairness = compute_st_loss(s, y_hat)\n",
    "        t_loss = 10000*budget_penalty(efforts, budget*0.95) + imp_w*improvement_loss + lt_w*lt_fairness + st_w*st_fairness + 20 * torch.relu(compute_st_loss(s, y_hat[:, -1].unsqueeze(1)) - 0.05)\n",
    "        \n",
    "        t_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            df = pd.DataFrame(t0)\n",
    "            df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "            df.to_csv('taiwan.csv')   \n",
    "            data_module = vaca_dataset(1, t0, device, 'taiwan')\n",
    "            val_loader = data_module.val_dataloader()\n",
    "            data_module.batch_size = 1\n",
    "            val_s = tensor(val_loader.dataset.X[:,0]).view(len(val_loader.dataset), 1).to(device)\n",
    "            val_loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "            val_efforts = net(tensor(val_loader.dataset.X).view(len(val_loader.dataset), 1, int(x_dim*2)+3).to(device))\n",
    "            val_y, val_x, val_data = intervene(data_module, val_loader, model_vaca, generator, clf, val_efforts, seq_len, 'taiwan', device)\n",
    "            val_loss = 10000*budget_penalty(val_efforts, budget*0.95) + imp_w*loss_fn(val_y, torch.ones_like(val_y)) + lt_w*compute_distance_loss(val_data) + st_w*compute_st_loss(val_s, val_y) + 20 * torch.relu(compute_st_loss(val_s, val_y[:, -1].unsqueeze(1)) - 0.05)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        df = pd.DataFrame(t0)\n",
    "        df = df.set_axis(['SEX', 'LIMIT_BAL_1', 'PAY_AMT1_1', 'PAY_AMT2_1', 'Y_1', 'LIMIT_BAL_2', 'PAY_AMT1_2', 'PAY_AMT2_2', 'Y_2'], axis=1)\n",
    "        df.to_csv('taiwan.csv')   \n",
    "        epoch_end = time.time()\n",
    "        hist_loss.append(t_loss.item())\n",
    "        if (val_loss < best_loss and epoch > 0):\n",
    "            best_loss = val_loss\n",
    "            patience = 0\n",
    "            torch.save(net.state_dict(), efforts_path)\n",
    "            save_checkpoint(epoch, net, optimizer, best_loss, checkpoint_path)\n",
    "            e = epoch\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            patience += 1\n",
    "            if (patience > 10 and 10000*budget_penalty(efforts, budget*.95) < 0.01):\n",
    "                print('Early stopping')\n",
    "                print(f' Last save: {e}')\n",
    "                break\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch: 4.0f} | Total Loss: {t_loss: 8.4f} | Budget Loss: {10000*budget_penalty(efforts, budget*.95) : 3.2f} | Long Fairness Loss: {lt_fairness: 3.4f} | Improvement Loss: {improvement_loss: 3.4f} | Time: {epoch_end - epoch_start: 3.1f} | Patience: {patience}')\n",
    "\n",
    "    print('Last saved epoch: ', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "og_fairness = np.empty((len(seeds), seq_len))\n",
    "n = 200\n",
    "model_fairness_lstm = np.empty((len(seeds), seq_len))\n",
    "short_term_og = np.empty((len(seeds), seq_len))\n",
    "short_term_fairness = np.empty((len(seeds), seq_len))   \n",
    "dataset = 'taiwan'\n",
    "loader = data_module.total_dataloader()\n",
    "b_list = [1, 2, 3, 4]\n",
    "for _b in b_list:\n",
    "    budget = _b*seq_len\n",
    "    net = effortLSTM(data_module, loader, seq_len, device)\n",
    "    efforts_lstm = torch.empty(len(seeds), n, seq_len-1, len(loader.dataset.nodes_to_intervene))\n",
    "    efforts_path = log_dir / (f\"LSTM_effort_model\" + dataset + \"_budget_\" + str(budget) + \"_eps_\" + str(eps) + \"_b_\" + str(bet) + \".pth\")\n",
    "    net.load_state_dict(torch.load(efforts_path, map_location=device))\n",
    "    for seed in seeds:\n",
    "        t1 = time.time()\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "        gan_noise = torch.randn(len(s0), seq_len-1, x_dim).to(device)\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "        s0 = torch.tensor(s0, dtype=torch.float32).view(len(s0),1).to(device)\n",
    "        x_gan, _, y_gan = generator(x0, gan_noise, s0, unfair_clf.to(device))\n",
    "        p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "        #\n",
    "        for i in range(seq_len):\n",
    "            gan_output = torch.cat([s0, x_gan[:, i, :], y_gan[:, i, :]], dim=1)\n",
    "            short_loss = compute_st_loss(s0, y_gan[:, i, :])\n",
    "            distance = compute_distance_loss(gan_output.to(device))\n",
    "            short_term_og[seed-2031, i] = short_loss.item()\n",
    "            og_fairness[seed-2031, i] = distance.item()\n",
    "        data_module = vaca_dataset(1, y0, device, 'taiwan')\n",
    "        loader = data_module.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        efforts = net(tensor(loader.dataset.X).view(len(loader.dataset), 1, int(x_dim*2)+3).to(device))\n",
    "        efforts_lstm[seed-2031] = efforts.view(len(loader), seq_len-1, 2).detach().cpu()\n",
    "        y_hat, x_hat, data = intervene(data_module, loader, model_vaca, generator, clf, efforts, seq_len, dataset, device)\n",
    "        for i in range(seq_len-1):\n",
    "            data_t = torch.cat([data[:, 0].view(len(s0),1).to(device), x_hat[:,i,:].to(device), y_hat[:,i].view(len(loader), 1).to(device)], dim=1)\n",
    "            short_loss = compute_st_loss(data[:, 0].view(n,1).to(device), y_hat[:, i].view(n,1).to(device))\n",
    "            distance = compute_distance_loss(data_t)\n",
    "            model_fairness_lstm[seed-2031, i+1] = distance.item()\n",
    "            short_term_fairness[seed-2031, i+1] = short_loss.item()\n",
    "\n",
    "        t2 = time.time()\n",
    "        print(f'Seed: {seed} | Time: {t2 - t1: 4.2f}')\n",
    "    np.save('efforts_budget_lstm_' + dataset + '_' + str(budget) + '.npy', efforts_lstm)\n",
    "    np.save('lt_fairness_budget_' + dataset + '_'+str(budget)+'.npy', model_fairness_lstm)\n",
    "    np.save('st_fairness_budget_' + dataset + '_'+str(budget)+'.npy', short_term_fairness)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "og_fairness = np.empty((len(seeds), seq_len))\n",
    "n = 200\n",
    "model_fairness_lstm = np.empty((len(seeds), seq_len))\n",
    "short_term_og = np.empty((len(seeds), seq_len))\n",
    "short_term_fairness = np.empty((len(seeds), seq_len))   \n",
    "dataset = 'taiwan'\n",
    "loader = data_module.total_dataloader()\n",
    "net = effortNN(data_module, loader, seq_len, device)\n",
    "for b in [1, 2, 3, 4]:\n",
    "    budget = b*seq_len\n",
    "    efforts_lstm = torch.empty(len(seeds), n, seq_len-1, len(loader.dataset.nodes_to_intervene))\n",
    "    efforts_path = log_dir / (f\"NN_effort_model\" + dataset + \"_budget_\" + str(budget) + \"_eps_\" + str(eps) + \"_b_\" + str(bet) + \".pth\")\n",
    "    net.load_state_dict(torch.load(efforts_path, map_location=device))\n",
    "    for seed in seeds:\n",
    "        t1 = time.time()\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "        gan_noise = torch.randn(len(s0), seq_len-1, x_dim).to(device)\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "        s0 = torch.tensor(s0, dtype=torch.float32).view(len(s0),1).to(device)\n",
    "        x_gan, _, y_gan = generator(x0, gan_noise, s0, unfair_clf.to(device))\n",
    "        p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "        #\n",
    "        for i in range(seq_len):\n",
    "            gan_output = torch.cat([s0, x_gan[:, i, :], y_gan[:, i, :]], dim=1)\n",
    "            short_loss = compute_st_loss(s0, y_gan[:, i, :])\n",
    "            distance = compute_distance_loss(gan_output.to(device))\n",
    "            short_term_og[seed-2031, i] = short_loss.item()\n",
    "            og_fairness[seed-2031, i] = distance.item()\n",
    "        data_module = vaca_dataset(1, y0, device, 'taiwan')\n",
    "        loader = data_module.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        efforts = net(tensor(loader.dataset.X).view(len(loader.dataset), 1, int(x_dim*2)+3).to(device))\n",
    "        efforts_lstm[seed-2031] = efforts.view(len(loader), seq_len-1, 2).detach().cpu()\n",
    "        y_hat, x_hat, data = intervene(data_module, loader, model_vaca, generator, clf, efforts, seq_len, dataset, device)\n",
    "        for i in range(seq_len-1):\n",
    "            data_t = torch.cat([data[:, 0].view(len(s0),1).to(device), x_hat[:,i,:].to(device), y_hat[:,i].view(len(loader), 1).to(device)], dim=1)\n",
    "            short_loss = compute_st_loss(data[:, 0].view(n,1).to(device), y_hat[:, i].view(n,1).to(device))\n",
    "            distance = compute_distance_loss(data_t)\n",
    "            model_fairness_lstm[seed-2031, i+1] = distance.item()\n",
    "            short_term_fairness[seed-2031, i+1] = short_loss.item()\n",
    "\n",
    "        t2 = time.time()\n",
    "        print(f'Seed: {seed} | Time: {t2 - t1: 4.2f}')\n",
    "\n",
    "    np.save('efforts_budget_nn_' + dataset + '_' + str(budget) + '.npy', efforts_lstm)\n",
    "    np.save('nn_lt_fairness_budget_' + dataset + '_'+str(budget)+'.npy', model_fairness_lstm)\n",
    "    np.save('nn_st_fairness_budget_' + dataset + '_'+str(budget)+'.npy', short_term_fairness)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline import *\n",
    "\n",
    "s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=100)\n",
    "x, y, unfair_clf = sequential_data(s0, x0, y0, seq_len, l=epsilon, noise_factor=0.1, seed=100, ground_truth=False)\n",
    "model = logReg(x_dim + 1).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)[0:int(n*(1-test_size)), :]\n",
    "test_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)[int(n*(1-test_size)):, :]\n",
    "\n",
    "train_dataset = SimpleDataset(train_data[:, :-1], train_data[:, -1])\n",
    "test_dataset = SimpleDataset(test_data[:, :-1], test_data[:, -1])\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_model = trainer_fb_fair(model, train_loader, [1,2], optim, device, 50, 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logReg(x_dim + 1).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "be_model = trainer_bounded_effort(model, train_loader, [1,2], optim, device, 50, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logReg(x_dim + 1).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "iler_model = trainer_iler(model, train_loader, [1,2], optim, device, 50, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logReg(x_dim + 1).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "er_model = trainer_er(model, train_loader, [1,2], optim, device, 50, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logReg(x_dim + 1).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "dp_model = trainer_dp_fair(model, train_loader, optim, device, 50, 0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "n = 200\n",
    "EI_fairness = np.empty((len(seeds), seq_len))\n",
    "efforts_baseline = []\n",
    "data = np.empty((n, seq_len, x_dim + 2))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "EI_accuracy = np.empty((len(seeds), seq_len))\n",
    "EI_short_term = np.empty((len(seeds), seq_len))\n",
    "b_list = [1, 2, 3, 4]\n",
    "for _b in b_list:\n",
    "    budget = _b* seq_len\n",
    "\n",
    "    for seed in seeds:\n",
    "        s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "        test_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)\n",
    "        test_dataset = SimpleDataset(test_data[:, :-1].detach().cpu().numpy(), test_data[:, -1].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        torch.manual_seed(seed)\n",
    "        z_mb = torch.randn(len(s0), 1, x_dim).to(device)\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "        s0 = torch.tensor(s0, dtype=torch.float32).view(len(s0),1).to(device)\n",
    "        distance = compute_distance_loss(test_data.to(device))\n",
    "        EI_fairness[seed-2031, 0] = distance.item()    \n",
    "        _y = ei_model.predict(s0, x0)\n",
    "        gt_y = clf.predict(s0, x0)\n",
    "        gt_y = tensor(gt_y).to(device)\n",
    "        EI_accuracy[seed-2031, 0] = loss_fn(_y, gt_y.to(device)).item()\n",
    "        EI_short_term[seed-2031, 0] = compute_st_loss(s0, gt_y).item()\n",
    "            \n",
    "        x_gan, _, y_gan = generator(x0, z_mb, s0, ei_model.to(device))\n",
    "        p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "        datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "        loader = datamodule.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        new_budget = tensor([_b])\n",
    "        for i in range(1, seq_len):\n",
    "            _, e, _data = test_fb_fair(ei_model, test_loader, [1,2], device, delta_effort=new_budget.item())\n",
    "            effort_indices = ((test_loader.dataset.labels < 1).squeeze()).nonzero()\n",
    "            efforts = torch.zeros([len(test_loader.dataset), int((x_dim)*2+3)]).to(device)\n",
    "            efforts[effort_indices[0], x_dim+2:x_dim+4] = e[:, [1, 2]].to(device)\n",
    "            x_gan_input = intervention_step(loader, model_vaca, datamodule.test_dataset.nodes_to_intervene, efforts, datamodule.test_dataset.nodes_list)\n",
    "            x_gan_input = x_gan_input.to(device)\n",
    "            _s = torch.tensor(test_loader.dataset.features[:, 0]).to(device)\n",
    "            _y = ei_model.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = clf.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = tensor(gt_y).to(device)\n",
    "            \n",
    "            distance = compute_distance_loss(torch.cat([_s.unsqueeze(1), x_gan_input.squeeze(), gt_y], dim=1).to(device))\n",
    "            EI_fairness[seed-2031, i] = distance.item()\n",
    "            new_budget = _b+(_b-torch.mean(torch.sum(torch.abs(e), dim=1)))\n",
    "            data[:, i, 1:-1] = x_gan_input.squeeze().detach().cpu().numpy()\n",
    "            data[:, i, 0] = _s.detach().cpu().numpy()\n",
    "            data[:, i, -1] = gt_y.squeeze().detach().cpu().numpy()\n",
    "            acc = loss_fn(_y, gt_y.to(device))\n",
    "            EI_accuracy[seed-2031, i] = acc.item()\n",
    "            EI_short_term[seed-2031, i] = compute_st_loss(_s, gt_y).item()\n",
    "            z_mb = torch.randn(len(_s), 1, x_dim).to(device)\n",
    "            x_gan, _, y_gan = generator(x_gan_input.squeeze().to(device), z_mb, _s.unsqueeze(1), ei_model.to(device))\n",
    "            p_to_csv(_s.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "            datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "            loader = datamodule.total_dataloader()\n",
    "            loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "            test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "            test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "            efforts_baseline.append(e[:, [1, 2]])\n",
    "    np.save('ei_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', EI_fairness)\n",
    "    np.save('ei_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', EI_short_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "n = 200\n",
    "ER_fairness = np.empty((len(seeds), seq_len))\n",
    "efforts_baseline = []\n",
    "data = np.empty((n, seq_len, x_dim + 2))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "ER_accuracy = np.empty((len(seeds), seq_len))\n",
    "ER_short_term = np.empty((len(seeds), seq_len))\n",
    "b_list = [1, 2, 3, 4]\n",
    "for _b in b_list:\n",
    "    \n",
    "    budget = _b* seq_len\n",
    "\n",
    "    for seed in seeds:\n",
    "        s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "        test_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)\n",
    "        test_dataset = SimpleDataset(test_data[:, :-1].detach().cpu().numpy(), test_data[:, -1].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        torch.manual_seed(seed)\n",
    "        z_mb = torch.randn(len(s0), 1, x_dim).to(device)\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "        s0 = torch.tensor(s0, dtype=torch.float32).view(len(s0),1).to(device)\n",
    "        distance = compute_distance_loss(test_data.to(device))\n",
    "        ER_fairness[seed-2031, 0] = distance.item()    \n",
    "        _y = ei_model.predict(s0, x0)\n",
    "        gt_y = clf.predict(s0, x0)\n",
    "        gt_y = tensor(gt_y).to(device)\n",
    "        ER_accuracy[seed-2031, 0] = loss_fn(_y, gt_y.to(device)).item()\n",
    "        ER_short_term[seed-2031, 0] = compute_st_loss(s0, gt_y).item()\n",
    "            \n",
    "        x_gan, _, y_gan = generator(x0, z_mb, s0, ei_model.to(device))\n",
    "        p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "        datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "        loader = datamodule.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        new_budget = tensor([_b])\n",
    "        for i in range(1, seq_len):\n",
    "            _, e, _data, _ = test_er(er_model, test_loader, [1,2], device, delta_effort=new_budget.item())\n",
    "            effort_indices = ((test_loader.dataset.labels < 1).squeeze()).nonzero()\n",
    "            efforts = torch.zeros([len(test_loader.dataset), int((x_dim)*2+3)]).to(device)\n",
    "            efforts[effort_indices[0], x_dim+2:x_dim+4] = e[:, [1, 2]].to(device)\n",
    "            x_gan_input = intervention_step(loader, model_vaca, datamodule.test_dataset.nodes_to_intervene, efforts, datamodule.test_dataset.nodes_list)\n",
    "            x_gan_input = x_gan_input.to(device)\n",
    "            _s = torch.tensor(test_loader.dataset.features[:, 0]).to(device)\n",
    "            _y = er_model.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = clf.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = tensor(gt_y).to(device)\n",
    "            \n",
    "            distance = compute_distance_loss(torch.cat([_s.unsqueeze(1), x_gan_input.squeeze(), gt_y], dim=1).to(device))\n",
    "            ER_fairness[seed-2031, i] = distance.item()\n",
    "            new_budget = _b+(_b-torch.mean(torch.sum(torch.abs(e), dim=1)))\n",
    "            data[:, i, 1:-1] = x_gan_input.squeeze().detach().cpu().numpy()\n",
    "            data[:, i, 0] = _s.detach().cpu().numpy()\n",
    "            data[:, i, -1] = gt_y.squeeze().detach().cpu().numpy()\n",
    "            acc = loss_fn(_y, gt_y.to(device))\n",
    "            ER_accuracy[seed-2031, i] = acc.item()\n",
    "            ER_short_term[seed-2031, i] = compute_st_loss(_s, gt_y).item()\n",
    "            z_mb = torch.randn(len(_s), 1, x_dim).to(device)\n",
    "            x_gan, _, y_gan = generator(x_gan_input.squeeze().to(device), z_mb, _s.unsqueeze(1), er_model.to(device))\n",
    "            p_to_csv(_s.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "            datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "            loader = datamodule.total_dataloader()\n",
    "            loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "            test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "            test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "\n",
    "    np.save('er_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', ER_fairness)\n",
    "    np.save('er_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', ER_short_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "n = 200\n",
    "ILER_fairness = np.empty((len(seeds), seq_len))\n",
    "efforts_baseline = []\n",
    "data = np.empty((n, seq_len, x_dim + 2))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "ILER_accuracy = np.empty((len(seeds), seq_len))\n",
    "ILER_short_term = np.empty((len(seeds), seq_len))\n",
    "b_list = [1, 2, 3, 4]\n",
    "for b in b_list:\n",
    "    budget = _b* seq_len\n",
    "\n",
    "    for seed in seeds:\n",
    "        s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "        test_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)\n",
    "        test_dataset = SimpleDataset(test_data[:, :-1].detach().cpu().numpy(), test_data[:, -1].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        torch.manual_seed(seed)\n",
    "        z_mb = torch.randn(len(s0), 1, x_dim).to(device)\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "        s0 = torch.tensor(s0, dtype=torch.float32).view(len(s0),1).to(device)\n",
    "        distance = compute_distance_loss(test_data.to(device))\n",
    "        ILER_fairness[seed-2031, 0] = distance.item()    \n",
    "        _y = iler_model.predict(s0, x0)\n",
    "        gt_y = clf.predict(s0, x0)\n",
    "        gt_y = tensor(gt_y).to(device)\n",
    "        ILER_accuracy[seed-2031, 0] = loss_fn(_y, gt_y.to(device)).item()\n",
    "        ILER_short_term[seed-2031, 0] = compute_st_loss(s0, gt_y).item()\n",
    "            \n",
    "        x_gan, _, y_gan = generator(x0, z_mb, s0, iler_model.to(device))\n",
    "        p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "        datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "        loader = datamodule.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        new_budget = tensor([_b])\n",
    "        for i in range(1, seq_len):\n",
    "            _, e, _data, _ = test_iler(iler_model, test_loader, [1,2], device, delta_effort=new_budget.item())\n",
    "            effort_indices = ((test_loader.dataset.labels < 1).squeeze()).nonzero()\n",
    "            efforts = torch.zeros([len(test_loader.dataset), int((x_dim)*2+3)]).to(device)\n",
    "            efforts[effort_indices[0], x_dim+2:x_dim+4] = e[:, [1, 2]].to(device)\n",
    "            x_gan_input = intervention_step(loader, model_vaca, datamodule.test_dataset.nodes_to_intervene, efforts, datamodule.test_dataset.nodes_list)\n",
    "            x_gan_input = x_gan_input.to(device)\n",
    "            _s = torch.tensor(test_loader.dataset.features[:, 0]).to(device)\n",
    "            _y = iler_model.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = clf.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = tensor(gt_y).to(device)\n",
    "            \n",
    "            distance = compute_distance_loss(torch.cat([_s.unsqueeze(1), x_gan_input.squeeze(), gt_y], dim=1).to(device))\n",
    "            ILER_fairness[seed-2031, i] = distance.item()\n",
    "            new_budget = _b+(_b-torch.mean(torch.sum(torch.abs(e), dim=1)))\n",
    "            data[:, i, 1:-1] = x_gan_input.squeeze().detach().cpu().numpy()\n",
    "            data[:, i, 0] = _s.detach().cpu().numpy()\n",
    "            data[:, i, -1] = gt_y.squeeze().detach().cpu().numpy()\n",
    "            acc = loss_fn(_y, gt_y.to(device))\n",
    "            ILER_accuracy[seed-2031, i] = acc.item()\n",
    "            ILER_short_term[seed-2031, i] = compute_st_loss(_s, gt_y).item()\n",
    "            z_mb = torch.randn(len(_s), 1, x_dim).to(device)\n",
    "            x_gan, _, y_gan = generator(x_gan_input.squeeze().to(device), z_mb, _s.unsqueeze(1), iler_model.to(device))\n",
    "            p_to_csv(_s.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "            datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "            loader = datamodule.total_dataloader()\n",
    "            loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "            test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "            test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "    np.save('iler_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', ILER_fairness)\n",
    "    np.save('iler_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', ILER_short_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "n = 200\n",
    "BE_fairness = np.empty((len(seeds), seq_len))\n",
    "be_efforts_baseline = []\n",
    "data = np.empty((n, seq_len, x_dim + 2))\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "BE_accuracy = np.empty((len(seeds), seq_len))\n",
    "BE_short_term = np.empty((len(seeds), seq_len))\n",
    "b_list = [1, 2, 3, 4]\n",
    "for _b in b_list:\n",
    "    \n",
    "    budget = _b* seq_len\n",
    "\n",
    "    for seed in seeds:\n",
    "        s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "        test_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)\n",
    "        test_dataset = SimpleDataset(test_data[:, :-1].detach().cpu().numpy(), test_data[:, -1].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        torch.manual_seed(seed)\n",
    "        z_mb = torch.randn(len(s0), 1, x_dim).to(device)\n",
    "        x0 = torch.tensor(x0, dtype=torch.float32).to(device)\n",
    "        s0 = torch.tensor(s0, dtype=torch.float32).view(len(s0),1).to(device)\n",
    "        distance = compute_distance_loss(test_data.to(device))\n",
    "        BE_fairness[seed-2031, 0] = distance.item()    \n",
    "        _y = ei_model.predict(s0, x0)\n",
    "        gt_y = clf.predict(s0, x0)\n",
    "        gt_y = tensor(gt_y).to(device)\n",
    "        BE_accuracy[seed-2031, 0] = loss_fn(_y, gt_y.to(device)).item()\n",
    "        BE_short_term[seed-2031, 0] = compute_st_loss(s0, gt_y).item()\n",
    "            \n",
    "        x_gan, _, y_gan = generator(x0, z_mb, s0, ei_model.to(device))\n",
    "        p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "        datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "        loader = datamodule.total_dataloader()\n",
    "        loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "        test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        new_budget = tensor([b])\n",
    "        for i in range(1, seq_len):\n",
    "            _, e, _data = test_bounded_effort(be_model, test_loader, [1,2], device=device, delta_effort=new_budget.item())\n",
    "            effort_indices = ((test_loader.dataset.labels < 1).squeeze()).nonzero()\n",
    "            efforts = torch.zeros([len(test_loader.dataset), int((x_dim)*2+3)]).to(device)\n",
    "            efforts[effort_indices[0], x_dim+2:x_dim+4] = e[:, [1, 2]].to(device)\n",
    "            x_gan_input = intervention_step(loader, model_vaca, datamodule.test_dataset.nodes_to_intervene, efforts, datamodule.test_dataset.nodes_list)\n",
    "            x_gan_input = x_gan_input.to(device)\n",
    "            _s = torch.tensor(test_loader.dataset.features[:, 0]).to(device)\n",
    "            _y = be_model.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = clf.predict(_s.unsqueeze(1), x_gan_input.squeeze())\n",
    "            gt_y = tensor(gt_y).to(device)\n",
    "            \n",
    "            distance = compute_distance_loss(torch.cat([_s.unsqueeze(1), x_gan_input.squeeze(), _y], dim=1).to(device))\n",
    "            BE_fairness[seed-2031, i] = distance.item()\n",
    "            new_budget = b+(b+torch.mean(torch.sum(torch.abs(e), dim=1)))\n",
    "            data[:, i, 1:-1] = x_gan_input.squeeze().detach().cpu().numpy()\n",
    "            data[:, i, 0] = _s.detach().cpu().numpy()\n",
    "            data[:, i, -1] = gt_y.squeeze().detach().cpu().numpy()\n",
    "            acc = loss_fn(_y, gt_y.to(device))\n",
    "            BE_accuracy[seed-2031, i] = acc.item()\n",
    "            BE_short_term[seed-2031, i] = compute_st_loss(_s, _y).item()\n",
    "            z_mb = torch.randn(len(_s), 1, x_dim).to(device)\n",
    "            x_gan, _, y_gan = generator(x_gan_input.squeeze().to(device), z_mb, _s.unsqueeze(1), be_model.to(device))\n",
    "            p_to_csv(s0.reshape(n,1), x_gan[:, 0:2, :], y_gan[:, 0:2, :], 'taiwan')\n",
    "            datamodule = vaca_dataset(1, y0, device, 'taiwan')\n",
    "            loader = datamodule.total_dataloader()\n",
    "            loader.dataset.set_intervention({'LIMIT_BAL_2': 0, 'PAY_AMT1_2': 0}, is_noise=True)\n",
    "            test_dataset = SimpleDataset(torch.cat([s0, x_gan[:, -1, :]], dim=1).detach().cpu().numpy(), y_gan[:, -1, :].detach().cpu().numpy())\n",
    "            test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "            be_efforts_baseline.append(e[:, [1, 2]])\n",
    "    np.save('be_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', BE_fairness)\n",
    "    np.save('be_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy', BE_short_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(2031, 2061))\n",
    "n = 200\n",
    "DP_fairness = np.empty((len(seeds), seq_len))\n",
    "data = np.empty((n, seq_len, x_dim + 2))\n",
    "DP_short_term = np.empty((len(seeds), seq_len))\n",
    "\n",
    "for seed in seeds:\n",
    "    s0, x0, y0, unfair_clf = sample_taiwan(file_path, int(n/2), seq_len, seed=seed)\n",
    "    test_data = torch.cat([tensor(s0.reshape(n, 1)), tensor(x0), tensor(y0).reshape(n,1)], dim=1)\n",
    "    test_dataset = SimpleDataset(test_data[:, :-1].detach().cpu().numpy(), test_data[:, -1].detach().cpu().numpy())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "    torch.manual_seed(seed)\n",
    "    new_budget = tensor([1])\n",
    "    for i in range(seq_len):\n",
    "        _, _data = test_dp_fair(dp_model, test_loader, device)\n",
    "        _s, _x, _y = torch.split(_data, [1, x_dim, 1], dim=1)\n",
    "        gt_y = clf(_s.to(device), _x.to(device))\n",
    "        DP_short_term[seed-2031, i] = compute_st_loss(_s, gt_y).item()\n",
    "        z_mb = torch.randn(len(_s), 1, x_dim).to(device)\n",
    "        gan_output = generator(_x.to(device), z_mb, _s, dp_model.to(device))\n",
    "        gan_x = gan_output[0][:, -1, :]\n",
    "        gan_y = gan_output[1][:, -1, :]\n",
    "        test_dataset = SimpleDataset(torch.cat([_s, gan_x], dim=1).detach().cpu().numpy(), gan_y.detach().cpu().numpy())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=n, shuffle=False)\n",
    "        distance = compute_distance_loss(torch.cat([_s, gan_x, gan_y], dim=1).to(device))\n",
    "        DP_fairness[seed-2031, i] = distance.item()\n",
    "        data[:, i, :] = torch.cat([_s, gan_x, gan_y], dim=1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list = [1, 2, 3, 4]\n",
    "og_fairness = np.load('og_fairness.npy')\n",
    "og_fairness_tl = np.mean(og_fairness, axis=0)\n",
    "timeline = range(seq_len)\n",
    "dataset = 'taiwan'\n",
    "\n",
    "\n",
    "for _b in b_list:\n",
    "    budget = _b * seq_len\n",
    "    model_fairness_lstm_clf = np.load('lt_fairness_clf_budget_' + str(budget) + '.npy')\n",
    "    model_fairness_lstm = np.load('lt_fairness_budget_' + str(budget) + '.npy')\n",
    "    model_fairness_tl_lstm = np.mean(model_fairness_lstm, axis=0)\n",
    "    model_fairness_tl_lstm[0] = og_fairness_tl[0]\n",
    "    EI_fairness = np.load('ei_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    BE_fairness = np.load('be_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    ER_fairness = np.load('er_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    ILER_fairness = np.load('iler_lt_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    EI_fairness_tl = np.mean(EI_fairness, axis=0)\n",
    "    BE_fairness_tl = np.mean(BE_fairness, axis=0)\n",
    "    DP_fairness_tl = np.mean(DP_fairness, axis=0)\n",
    "    ER_fairness_tl = np.mean(ER_fairness, axis=0)\n",
    "    ILER_fairness_tl = np.mean(ILER_fairness, axis=0)\n",
    "    EI_fairness_tl[0] = og_fairness_tl[0]\n",
    "    DP_fairness_tl[0] = og_fairness_tl[0]\n",
    "    ER_fairness_tl[0] = og_fairness_tl[0]\n",
    "    ILER_fairness_tl[0] = og_fairness_tl[0]\n",
    "    BE_fairness_tl[0] = og_fairness_tl[0]\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(timeline, og_fairness_tl, color='blue', label='Base', marker=\"*\")\n",
    "    ax.plot(timeline, model_fairness_tl_lstm, color='orange', label='SCARF', marker=\"o\")\n",
    "    ax.plot(timeline, EI_fairness_tl, color='red', label='EI', marker=\"s\")\n",
    "    ax.plot(timeline, DP_fairness_tl, color='purple', label='DP', marker=\"v\")\n",
    "    ax.plot(timeline, BE_fairness_tl, color='green', label='BE', marker=\"D\")\n",
    "    ax.plot(timeline, ER_fairness_tl, color='pink', label='ER', marker=\"P\")\n",
    "    ax.plot(timeline, ILER_fairness_tl, color='gray', label='ILER', marker=\"H\")\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.legend(fontsize=14, loc='upper left')\n",
    "    plt.savefig(\"Taiwan Results - 30 seeds averaged budget = \" + str(budget) + \", epsilon = \"+eps+\", beta = \"+bet+\".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_short_term = np.load('st_fairness_og.npy')\n",
    "og_short_term_tl = np.mean(og_short_term, axis=0)\n",
    "b_list = [1, 2, 3, 4]\n",
    "for _b in b_list:\n",
    "    budget = _b * seq_len\n",
    "    short_term_fairness = np.load('st_fairness_budget_' + str(budget) + '.npy')\n",
    "    short_term_lstm_tl = np.mean(short_term_fairness, axis=0)\n",
    "    short_term_lstm_tl[0] = og_short_term_tl[0]\n",
    "    ILER_short_term = np.load('iler_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    ER_short_term = np.load('er_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    EI_short_term = np.load('ei_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "\n",
    "    BE_short_term = np.load('be_st_fairness_budget_' + dataset + '_budget_' + str(budget) + '.npy')\n",
    "    EI_short_tl = np.mean(EI_short_term, axis=0)\n",
    "    DP_short_tl = np.mean(DP_short_term, axis=0)\n",
    "    BE_short_term_tl = np.mean(BE_short_term, axis=0)\n",
    "    ER_short_term_tl = np.mean(ER_short_term, axis=0)\n",
    "    ILER_short_term_tl = np.mean(ILER_short_term, axis=0)\n",
    "    EI_short_tl[0] = og_short_term_tl[0]\n",
    "    DP_short_tl[0] = og_short_term_tl[0]\n",
    "    BE_short_term_tl[0] = og_short_term_tl[0]\n",
    "    ER_short_term_tl[0] = og_short_term_tl[0]\n",
    "    ILER_short_term_tl[0] = og_short_term_tl[0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.plot(timeline, og_short_term_tl, color='blue', label='Base', marker = \"*\")\n",
    "    ax.plot(timeline, short_term_lstm_tl, color='orange', label='SCARF', marker = \"o\")\n",
    "    ax.plot(timeline, EI_short_tl, color='red', label='EI', marker = \"s\")\n",
    "    ax.plot(timeline, DP_short_tl, color='purple', label='DP', marker = \"v\")\n",
    "    ax.plot(timeline, BE_short_term_tl, color='green', label='BE', marker = \"D\")\n",
    "    # plt.plot(timeline, short_term_clf_tl, color='orange', label='Post Interventions Short Term Fairness LSTM CLF', marker = \"x\")\n",
    "    ax.plot(timeline, ER_short_term_tl, color='pink', label='ER', marker = \"P\")\n",
    "    ax.plot(timeline, ILER_short_term_tl, color='gray', label='ILER', marker = \"H\")\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.legend(fontsize=14, loc = 'upper left')\n",
    "    plt.savefig(\"Taiwan Short Term Results - 30 seeds averaged budget = \" + str(budget) + \", epsilon = \"+eps+\", beta = \"+bet+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list = [1, 2, 3, 4]\n",
    "color_list = ['orange', 'green', 'blue', 'purple']\n",
    "og_short_term = np.load('st_fairness_og_toy.npy')\n",
    "og_fairness = np.load('og_fairness_toy.npy')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for idx, _b in enumerate(b_list):\n",
    "    lstm_fairness = np.load('lt_fairness_budget_' + str(_b*seq_len) + '.npy')\n",
    "    lstm_fairness_tl = np.mean(lstm_fairness, axis=0)\n",
    "    og_fairness_tl = np.mean(og_fairness, axis=0)\n",
    "    og_short_term_tl = np.mean(og_short_term, axis=0)\n",
    "    lstm_fairness_tl[0] = og_fairness_tl[0]\n",
    "    timeline = range(seq_len)\n",
    "    \n",
    "    ax.plot(timeline, lstm_fairness_tl, color=color_list[idx], label=f'LT - Budget {_b*seq_len}', marker = \"s\")\n",
    "    \n",
    "ax.tick_params(axis='both', labelsize=16)\n",
    "ax.legend(fontsize=14, loc = 'upper left')\n",
    "plt.savefig(f'Taiwan Budget Sensitivity Long Term.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list = [1, 2, 3, 4]\n",
    "color_list = ['orange', 'green', 'blue', 'purple']\n",
    "og_short_term = np.load('st_fairness_og_toy.npy')\n",
    "og_fairness = np.load('og_fairness_toy.npy')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for idx, _b in enumerate(b_list):\n",
    "    lstm_short_term = np.load('st_fairness_budget_' + str(_b*seq_len) + '.npy')\n",
    "    lstm_short_term_tl = np.mean(lstm_short_term, axis=0)\n",
    "    lstm_short_term_tl[0] = og_short_term_tl[0]\n",
    "    timeline = range(seq_len)\n",
    "    \n",
    "    ax.plot(timeline, lstm_short_term_tl, color=color_list[idx], label=f'ST - Budget {_b*seq_len}', marker = \"s\")\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=16)\n",
    "ax.legend(fontsize=14, loc = 'upper left')\n",
    "plt.savefig(f'Taiwan Budget Sensitivity Short Term.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_list = [1, 2, 3, 4]\n",
    "og_fairness = np.load('og_fairness.npy')\n",
    "og_short_term = np.load('st_fairness_og.npy')\n",
    "og_fairness_tl = np.mean(og_fairness, axis=0)\n",
    "og_short_term_tl = np.mean(og_short_term, axis=0)\n",
    "timeline = range(seq_len)\n",
    "for _b in b_list:\n",
    "    budget = _b * seq_len\n",
    "    lstm_fairness = np.load('lt_fairness_budget_' + str(budget) + '.npy')\n",
    "    lstm_fairness_tl = np.mean(lstm_fairness, axis=0)\n",
    "    lstm_st_fairness = np.load('st_fairness_budget_' + str(budget) + '.npy')\n",
    "    lstm_st_fairness_tl = np.mean(lstm_st_fairness, axis=0)\n",
    "    nn_fairness = np.load('nn_lt_fairness_budget_' + dataset + '_'+str(budget)+'.npy')\n",
    "    nn_fairness_tl = np.mean(nn_fairness, axis=0)\n",
    "    nn_st_fairness = np.load('nn_st_fairness_budget_' + dataset + '_'+str(budget)+'.npy')\n",
    "    nn_st_fairness_tl = np.mean(nn_st_fairness, axis=0)\n",
    "    lstm_fairness_tl[0] = og_fairness_tl[0]\n",
    "    lstm_st_fairness_tl[0] = og_short_term_tl[0]\n",
    "    nn_fairness_tl[0] = og_fairness_tl[0]\n",
    "    nn_st_fairness_tl[0] = og_short_term_tl[0]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(timeline, nn_fairness_tl, color='green', label='MLP', marker = \"s\")\n",
    "    ax.plot(timeline, nn_st_fairness_tl, color='green', linestyle='dashed', label='MLP - Short Term', marker = \"s\")\n",
    "    ax.plot(timeline, lstm_fairness_tl, color='orange', label='SCARF', marker = \"o\")\n",
    "    ax.plot(timeline, lstm_st_fairness_tl, color='orange', linestyle='dashed', label='SCARF - Short Term', marker = \"o\")\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.legend(fontsize=14, loc = 'upper left')\n",
    "    plt.savefig(f'Taiwan Long Term vs Short Term Fairness Ablation, Budget = {budget}.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
